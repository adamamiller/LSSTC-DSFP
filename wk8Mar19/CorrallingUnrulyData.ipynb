{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wranglin' –– Corraling Unruly Data\n",
    "One bit at a time\n",
    "=====\n",
    "\n",
    "**Version 0.1**\n",
    "\n",
    "By AA Miller 2019 Mar 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Webster's Dictionary$^\\ast$ defines wrangler as:\n",
    "\n",
    "**wrangler** noun\n",
    "\n",
    "wran·gler | raŋ-g(ə-)lər\n",
    "\n",
    "(short for horse-wrangler, probably partial translation of Mexican Spanish caballerango groom): a ranch hand who takes care of the saddle horses broadly : cowboy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$^\\ast$actually https://www.merriam-webster.com/dictionary/ - Webster's didn't define wrangler in the way I wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How then, as astronomers, are we all like cowhands?\n",
    "\n",
    "Data are often like horses in that: they all differ, rarely conform to a single standard set of behavior, and they love to eat hay.$^\\dagger$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$^\\dagger$I made that last one up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thus, in our efforts to better understand the Universe, we must often manipulate, coax, and, in some cases, force our data to \"behave.\" This involves a variety of tasks, such as: gathering, cleaning, matching, restructuring, transforming, filtering, combining, merging, verifying, and fixing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is a brief and unfortunate truth, there isn't a single person in the entire world that would organize data in *exactly* the same way that you would.\n",
    "\n",
    "As a result, you may find that data that are useful to you are not organized in an optimal fashion for use in your workflow/software.\n",
    "\n",
    "Hence: the need to wrangle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is one important and significant way in which our lives as astronomers are much better than the average data scientist: even though our data are \"worthless,\" virtually all of it is numbers.\n",
    "\n",
    "Furthermore, I contend that most astronomical data can easily be organized into a simple tabular structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Yesterday we joked that you could write a 10 page text document describing every individual galaxy in a galaxy cluster with 5000 members, but no one would ever do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nevertheless, as you will see during the exercises, even with relatively simple, small numerical data sets there is a need for wrangling.\n",
    "\n",
    "And wrangling brings up a lot of issues..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider the following data set that contains the street names for my best friends from childhood:\n",
    "\n",
    "    ['Ewing', 'Isabella', 'Reese', 'Isabella', \n",
    "     'Thayer', 'Reese', 'Reese', 'Ewing', 'Reece']\n",
    "\n",
    "Do you notice anything interesting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Either my hometown has a street named \"Reese\"  and a street named \"Reece\", or the last entry was input incorrectly. \n",
    "\n",
    "If the later is true, then we have to raise the question of: what should we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For this particular data set, it would be possible to create a verification procedure to test for similar errors.\n",
    "\n",
    "1. Collect the names of every street in the city (from post office?)\n",
    "2. Confirm every instance in the data set has a counterpart.\n",
    "\n",
    "For any instances where this isn't the case, one could then intervene with a correction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This particular verification catches this street name error, but it doesn't correct for the possibility that the person doing the data entry may have been reading addresses really quickly and the third \"Reese\" entry should have actually said \"Lawndale.\"\n",
    "\n",
    "(verification is really hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This also raises a question about data provenance. \n",
    "\n",
    "Data provenance is a historical record of the data and its origins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you are making \"corrections\" to the data, then each and every one of those corrections should be reported (this is similar to the idea of logging from yesterday's databases talk). Ideally, these reports would live with the data so others could understand how things have changed.\n",
    "\n",
    "If you did change \"Reece\" to \"Reese\", anyone working with the data should be able to confirm those changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose now you wanted to use the same street name data set to estimate which street I lived on while growing up. \n",
    "\n",
    "One way to mathematically approach this problem would be to convert the streets in the data set to GPS coordinates, and then to perform a KDE of the PDF for the coordinates of where I lived. \n",
    "\n",
    "This too is a form of wrangling, because the data you have (street names) are not the data you need (coordinates). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why harp on this? \n",
    "\n",
    "In practice, data scientists (including astronomers) spend an unreasonable amount of time manipulating and quality checking data (some indsutry experts estimate that up to 80% of their time is spent warnglin').\n",
    "\n",
    "Today, we will work through several examples that require wrangling, while, hopefully, building some strategies to minimize the amount of time you spend on these tasks in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For completeness, I will mention that there is a famous canonical paper about [data wrangling](http://vis.stanford.edu/files/2011-Wrangler-CHI.pdf), which introduces the [`Wrangler`](http://vis.stanford.edu/wrangler/), a tool specifically designed to take heterogeneous (text) data, and provide a set of suggested operations/manipulations to  create a homogenous table amenable to standard statistical analysis. \n",
    "\n",
    "One extremely nice property of the `Wrangler` is that it records every operation performed on the data, ensuring high-fidelity reporting on the data provenance. We should do a better job of this in astronomy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Today, we are going to focus on `python` solutions to some specific astronomical data sets.\n",
    "\n",
    "Hopefully you learn some tricks to make your work easier in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 1) The Sins of Our Predecessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If at any point in your career you need to access archival infrared data, you will likely need to retrieve that information from the [NASA IPAC InfRed Science Archive](https://irsa.ipac.caltech.edu). IRSA houses the data for every major NASA IR mission, and several ground-based missions as well (e.g., 2MASS, IRTF). Whether you are sudying brown dwarfs, explosive transients, solar system objects, star-formation, galaxy evolution, Milky Way dust and the resulting extinction of extragalactic observations, or quasars (and much more) the IR plays a critical role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given the importance of IR observations, it stands to reason that IRSA would provide data in a simple to read format for modern machines, such as comma separated values or FITS binary tables...\n",
    "\n",
    "Right?..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Right?...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In fact, IRSA has created their own standard for storing data in a text file. If you haven't already, please download [irsa_catalog_WISE_iPTF14jg_search_results.tbl](https://northwestern.box.com/s/9lnpqwcmmke2f5wqptk2q1409kfydc2i), a file that is written in the standard IRSA format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*shameless plug alert!* iPTF14jg is a [really strange star](https://arxiv.org/pdf/1901.10693.pdf) that exhibited a large outburst that we still don't totally understand. The associated data file includes [NEOWISE](https://neowise.ipac.caltech.edu/) observations of the mid-IR evolution of this outburst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 1a**\n",
    "\n",
    "Using `pandas` read the data in the IRSA table file into a `DataFrame` object.\n",
    "\n",
    "*Hint 1* - you absolutely should look at the text file to develop a strategy to accomplish this goal.\n",
    "\n",
    "*Hint 2* - you may want to manipulate the text file so that it can more easily be read by `pandas`. **If you do this** be sure to copy the file to another name as you will want to leave the original intact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Use pandas to read in the IRSA data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That pure python solution is a bit annoying as it requires a for loop with a break, and specific knowledge about how IRSA tables handler data headers (hence the use of `linenum + 4` for `skiprows`). Alternatively, one could  manipulate the data file to read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 2nd solution - YOU DON'T NEED TO DO THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That truly wasn't all that better - as it required a bunch of clicks/text editor edits. (There are programs such as `sed` and `awk` that could be used to execute all the necessary edits from the command line, but that too is cumbersome and somewhat like the initial all `python` solution). \n",
    "\n",
    "If astronomers are creating data in a \"standard\" format, then it ought to be easy for other astronomers to access that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fortunately, in this particular case, there is an easy solution - [`astropy Tables`](http://docs.astropy.org/en/stable/table/). \n",
    "\n",
    "IRSA tables are so commonly used throughout the community, that the folks at `astropy` have created a convenience method for all of us to read in tables created in that particular (unusual?) format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 1b**\n",
    "\n",
    "Use [`Table.read()`](http://docs.astropy.org/en/stable/api/astropy.table.Table.html#astropy.table.Table.read) to read in `irsa_catalog_WISE_iPTF14jg_search_results.tbl` to an `astropy Table` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "\n",
    "Table.read( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A benefit to using this method, as opposed to `pandas`, is that data typing and data units are naturally read from the IRSA table and included with the associated columns. Thus, if you are uncertain if some brightness measurement is in magnitudes or Janskys, the `astropy Table` can report on that information.\n",
    "\n",
    "Unfortunately, `astropy` does *not* know about every strange formating decision that every astronomer has made at some point in their lives (as we are about to see...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 2) The Sins of Our Journals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unlike IRSA/IPAC, which uses a weird but nevertheless consistent format for data tables, data retrieved from Journal articles essentially follows no rules. In principle, tables in Journal articles are supposed to be provided in a machine readable format. In practice, as we are about to see, this is far from the case.\n",
    "\n",
    "For this particular wrangling case study we will focus on supernova light curves, a simple thing to report: time, filter, brightness, uncertainty on that brightness, that the community has nevertheless managed to mangle into some truly wild and difficult to parse forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "(Sorry for the heavy emphasis on time-domain examples - I'm pulling straight from my own life today, but the issues described here are not perfectly addressed by any subfield within the astro umbrella)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is the LaTeX-formatted version of Table 4 from [Miller et al. 2011](https://iopscience.iop.org/article/10.1088/0004-637X/730/2/80/meta):\n",
    "\n",
    "<img style=\"display: block; margin-left: auto; margin-right: auto\" src=\"images/Miller11_tbl4.png\" align=\\\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is a very simple table to interpret, no?\n",
    "\n",
    "Have a look at the [\"machine-readible\" file](https://northwestern.box.com/s/o9nxkr3o79xunfo2o7r8nnas2sskncrc) that ApJ provides for readers that might want to evaluate these photometric measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2a** \n",
    "\n",
    "Download the ApJ version of [Table 4](https://northwestern.box.com/s/o9nxkr3o79xunfo2o7r8nnas2sskncrc) from Miller et al. 2011 and read that table into either a `pandas DataFrame` or an `astropy Table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# read in the data from Miller et al 2011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That wasn't too terrible. But what if we consider a more typical light curve table, where there are loads of missing data, such as Table 2 from [Foley et al. 2009]():\n",
    "\n",
    "<img style=\"display: block; margin-left: auto; margin-right: auto\" src=\"images/Foley09_tbl2.png\" align=\\\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again, this table is straightforward to read, and it isn't hard to imagine how one could construct a machine-readable csv or other file from this information. But alas, this is not what is available from ApJ. So, we will need to figure out how to deal with both the missing data, \"...\", and the weird convention that many astronomers use where the uncertainties are (a) not reported in their own column, and (b) are not provided in the same units as the measurement itself. I can understand the former, but the later is somewhat baffling..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2b** \n",
    "\n",
    "Download the ApJ version of [Table 2](https://northwestern.box.com/s/imo8sc0rudfwaa4iaa9qr2rvcwtgvieb) from Foley et al. 2009 and read that table into either a `pandas DataFrame` or an `astropy Table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# read in the data from Foley et al 2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Okay - there is nothing elegant about that particular solution. But it works, and wranglin' ain't pretty. \n",
    "\n",
    "It is likely that you developed a solution that looks very different from this one, and that is fine. When data are provided in an unrulely format, the most important thing is to develop some method, any method, for converting the information into a useful format. Following whatever path you used above, it should now be easy to plot the light curve of SN 2008ha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 3) My Heart Will Go On\n",
    "\n",
    "Sometimes there is no difficultly whatsoever in reading in the data (as was the case in **Problems 1** and **2**), but instead the difficultly lies in wranglin' the data to be appropriate for the model that you are building.\n",
    "\n",
    "For the next problem we will work with the famous [Titanic survival](https://www.kaggle.com/c/titanic/data?) data set. If you haven't already, please [download the csv file](https://northwestern.box.com/s/hx6airvgb4mukxvztvhlzvedkq8480ug) with the relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Briefly, [the Titantic](https://thefilmcricket.files.wordpress.com/2012/04/film-titanic_clar.jpg) is a [famous](https://wallpapercave.com/wp/jrF8rQK.jpg) historical ship that was thought to be unsinkable. **Spoiler alert** it hit an iceberg and sank. The data in the Titanic data set includes information about 891 passengers from the Titanic, as well as whether or not they survived. The aim of this data set is to build a machine learning model to predict which passengers survived and which did not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The features include: \n",
    "\n",
    "|Feature    | Description |\n",
    "|:---------:|:--------------------------------------:|\n",
    "|PassengerId| Running index that describes the individual passengers|\n",
    "|Pclass| A proxy for socio-economic status (1 = Upper class, 2 = Middle Class, 3 = Lower Class)|\n",
    "|Name| The passenger's name|\n",
    "|Sex | The passenger's sex|\n",
    "|Age | The passenger's age - note age's ending in 0.5 are estimated |\n",
    "|SibSp| The sum of the passenger's sibblings and spouces on board|\n",
    "|Parch| The sum of the passenger's parents and children on board|\n",
    "|Ticket| The ticket number for the passenger|\n",
    "|Fare| The price paid for the ticket by th passenger|\n",
    "|Cabin| The Cabin in which the passenger stayed|\n",
    "|Embarked| The point of Origin for the Passenger: C = Cherbourg, S = Southampton, Q = Queenstown|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And of course, we are trying to predict:\n",
    "\n",
    "|Label    | Description |\n",
    "|:---------:|:--------------------------------------:|\n",
    "|Survived| 1 = yes; 0 = no|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3a**\n",
    "\n",
    "Read in the Titanic training data and create the `scikit-learn` standard `X` and `y` arrays to hold the features and the labels, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "titanic_df = pd.read_csv( # complete\n",
    "\n",
    "feat_list = list(titanic_df.columns)\n",
    "label = 'Survived'\n",
    "feat_list.remove(label)\n",
    "X = # complete\n",
    "y = # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we have the data in the appropriate `X` and `y` arrays, estimate the accuracy with which a [K nearest neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) classification model can predict whether or not a passenger would survive the Titanic disaster. Use $k=10$ fold cross validation for the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 3b**\n",
    "\n",
    "Train a $k=7$ nearest neighbors machine learning model on the Titanic training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier( # complete\n",
    "knn_clf.fit( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note - that should have failed! And for good reason - recall that `kNN` models measure the Euclidean distance between all points within the feature space. So, when considering the sex of a passenger, what is the *numerical* distance between male and female? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words, we need to wrangle this data before we can run the machine learning model. \n",
    "\n",
    "Most of the features in this problem are non-numeric (i.e. we are dealing with categorical features), and therefore we need to figure out how to include them in the `kNN` model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The first step when wrangling for machine learning is to figure out if anything can be thrown away. We certainly want to avoid including any uninformative features in the model. \n",
    "\n",
    "*If you haven't already, now would be a good time to create a new cell and examine the contents of the csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "titanic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 3c** \n",
    "\n",
    "Are there any features that *obviously* will not help with this classification problem?\n",
    "\n",
    "*If you answer yes - ignore those features moving forward*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Solution 3c**\n",
    "\n",
    "*write your solution here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given that we have both categorical and numeric features, let's start with the numerical features and see how well they can predict survival on the Titanic.\n",
    "\n",
    "One note - for now we are going to exclude `Age`, because as you saw when you examined the data, there are some passengers that do not have any age information. This problem, known as \"missing data\" is one that we will deal with before the end of this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 3d**\n",
    "\n",
    "How accurately can the numeric features, `Pclass`, `SibSp`, `Parch`, and `Fare` predict survival on the Titanic? Use a $k = 7$ Nearest Neighbors model, and estimate the model accuracy using 10-fold cross validation. \n",
    "\n",
    "*Hint 1 - you'll want to redefine your features vector `X`*\n",
    "\n",
    "*Hint 2 - you may find [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) from `scikit-learn` helpful*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X = # complete\n",
    "\n",
    "knn_clf = # complete\n",
    "\n",
    "cv_results = cross_val_score( # complete\n",
    "\n",
    "print('The accuracy from numeric features = {:.2f}%'.format(100*np.mean(cv_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An accuracy of 68% isn't particularly inspiring. But, there's a lot of important information that we are excluding. As far as the Titanic is concerned, Kate and Leo taught us that [female passengers are far more likely to survive](https://qph.fs.quoracdn.net/main-qimg-93eb36091c7eec872b891fa51dc5722b), while [male passengers are not](http://hoycinema.abc.es/Media/201602/03/titanic-kate-dicaprio--644x362.jpg). So, if we can include gender in the model then we may be able to achieve more accurate predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 3e**\n",
    "\n",
    "Create a new feature called `gender` that equals 1 for male passengers and 2 for female passengers. Add this feature to your dataframe, and include it in a `kNN` model with the other numeric features. Does the inclusion of this feature improve the 10-fold CV accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "gender = # complete\n",
    "# complete\n",
    "\n",
    "titanic_df['gender'] = gender\n",
    "\n",
    "X = # complete\n",
    "\n",
    "cv_results = cross_val_score( # complete\n",
    "\n",
    "print('The accuracy when including gender = {:.2f}%'.format(100*np.mean(cv_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A 14% improvement is pretty good! But, we can wrangle even more out of the gender feature. Recall that `kNN` models measure the Euclidean distance between sources, meaning the scale of each feature really matters. Given that the fare ranges from 0 up to 512.3292, the `kNN` model will see this feature as far more important than `gender`, for no other reason than the units that have been adopted. \n",
    "\n",
    "If women are far more likely to survive than men, then we want to be sure that gender is weighted at least the same as all the other features, which we can do with a minmax scaler. As a brief reminder - a minmax scaler scales all values of a feature to be between 0 and 1 by subtracting the minimum value of each feature and then dividing by the maximum minus the minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 3f**\n",
    "\n",
    "Scale all the features from the previous problem using a minmax scaler and evaluate the CV accuracy of the `kNN` model.\n",
    "\n",
    "*Hint - you may find [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) helpful*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "Xminmax = scaler.transform(X)\n",
    "\n",
    "knn_clf = # complete\n",
    "\n",
    "cv_results = cross_val_score( # complete\n",
    "\n",
    "print('The accuracy when scaling features = {:.2f}%'.format(100*np.mean(cv_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scaling the features leads to further improvement!\n",
    "\n",
    "Now turn your attention to another categorical feature, `Embarked`, which includes the point of origin for each passenger and has three categories, `S`, `Q`, and `C`. We need to convert these values to a numeric representation for inclusion in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 3g**\n",
    "\n",
    "Convert the categorical feature `Embarked` to a numeric representation, and add it to the `titanic_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# following previous example, set C = 0, S = 1, Q = 2\n",
    "\n",
    "porigin = # complete\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "titanic_df['porigin'] = porigin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But wait! Does this actually make sense?\n",
    "\n",
    "Our \"numerification\" has now introduced order where there previously was none. We are effectively telling the model that Cherbourg and Queenstown are far apart (not in distance but in terms of the similarity of the passengers that boarded the ship in each location), while each are equally close to Southampton. Is there actually any evidence to support this conclusion? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By definition categorical features do not have order (e.g., cat, dog, horse, elephant), and therefore we should not impose any when converting these features to numeric values for inclusion in our model. Instead, we should be creating a new set of binary features for every category within the feature set. Thus, `Embarked` will now need to be represented by 3 different features, where the feature `Queenstown` equals one for passengers that boarded there and zero for everyone else. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 3h**\n",
    "\n",
    "Complete the function below that will automatically create binary arrays for a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def create_bin_cat_feats(feature_array):\n",
    "    categories = np.unique(feature_array)\n",
    "    feat_dict = {}\n",
    "    for cat in categories:\n",
    "        # complete\n",
    "        # complete\n",
    "        # complete\n",
    "    return feat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3i**\n",
    "\n",
    "Use the `create_bin_cat_feats` function to convert the `Embarked` and `Sex`, yes we need to do this for `Sex` as well where we otherwise previously introduced order, categorical features to a numeric representation. Add these features to the `titanic_df` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "gender_dict = create_bin_cat_feats( # complete\n",
    "porigin_dict = create_bin_cat_feats( # complete\n",
    "\n",
    "for feat in gender_dict.keys():\n",
    "    titanic_df[feat] = gender_dict[feat]\n",
    "    \n",
    "for feat in porigin_dict.keys():\n",
    "    titanic_df[feat] = porigin_dict[feat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3j**\n",
    "\n",
    "Use the newly created `female`, `male`, `S`, `Q`, and `C` features in combination with the `Pclass`, `SibSp`, `Parch`, and `Fare` features to estimate the classification accuracy of a $k = 7$ nearest neighbors model with 10-fold cross validation.\n",
    "\n",
    "How does the addition of the point of origin feature affect the final model output?\n",
    "\n",
    "*Hint - don't forget to scale the features in the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = # complete\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "Xminmax = scaler.transform(X)\n",
    "\n",
    "cv_results = cross_val_score( # complete\n",
    "\n",
    "print('The accuracy with categorical features = {:.2f}%'.format(100*np.mean(cv_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The last thing we'd like to add to the model is the `Age` feature. Unfortunately, for 177 passengers we do not have a reported value for their age. This is a standard issue when building models known as \"missing data\" and this happens in astronomy all the time (for example, LSST is going to observe millions of L and T dwarfs that are easily detected in the $y$-band, but which do not have a detection in $u$-band)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are several different strategies for dealing with missing data. The first and most straightforward is to simply remove observations with missing data (note - to simplify this example I already did this by removing the 2 passengers from the training set that did not have an entry for `Embarked`). \n",
    "\n",
    "This strategy is perfectly fine if only a few sources have missing information (2/891 for `Embarked` - and none of the test set sources are missing `Embarked`). If, however, a significant fraction are missing data, this strategy would remove a lot of useful data from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you cannot remove the sources with missing data, then it is essential to ask the following question: \n",
    "\n",
    "Does the missing information have meaning? \n",
    "\n",
    "In the LSST L/T dwarf example, the lack of a $u$-band detection is meaningful: these stars are too faint for LSST. When this is the case, an indicator value (e.g., -999) allows the model to recognize the non-detection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the Titanic data, the lack of age information is not meaningful. Simply put, there are some passengers that did not have recorded ages. We will now show this to be the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 3k**\n",
    "\n",
    "Replace the unknown ages with a value of -999, and estimate the accuracy of the model via 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "age_impute = # complete\n",
    "# complete\n",
    "\n",
    "titanic_df['age_impute'] = age_impute\n",
    "\n",
    "X = # complete\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "Xminmax = scaler.transform(X)\n",
    "\n",
    "cv_results = cross_val_score( # complete\n",
    "\n",
    "print('The accuracy with -999 for missing ages = {:.2f}%'.format(100*np.mean(cv_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The accuracy of the model hasn't improved by adding the age information (even though we know children were more likely to survive than adults). \n",
    "\n",
    "Given that the missing ages don't have meaning, we need to develop alternative strategies for \"imputing\" the missing data. The most simple approach in this regard is to replace the missing values with the mean value of the feature distribution for sources that do have measurements (use the median if the distribution has significant outliers).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 3l**\n",
    "\n",
    "Replace the unknown ages with the mean age of passengers, and estiamte the accuracy of the model via 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "age_impute = # complete\n",
    "# complete\n",
    "\n",
    "titanic_df['age_impute'] = age_impute\n",
    "\n",
    "X = # complete\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "Xminmax = scaler.transform(X)\n",
    "\n",
    "cv_results = cross_val_score( # complete\n",
    "\n",
    "print('The accuracy with the mean for missing ages = {:.2f}%'.format(100*np.mean(cv_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using the mean age for missing values provides a marginal improvement over the models with no age information. Is there anything else we can do? Yes - we can build a machine learning model to predict the values of the missing data. So there will be a machine learning model within the final machine learning model. In order to predict ages, we will need to build a regression model. Simple algorithms include Linear or Logistic Regression, while more complex examples include `kNN` or random forest regression.\n",
    "\n",
    "I quickly tested the above four methods, and found that the `scikit-learn` [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) performs best when using the model defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Probem 3m**\n",
    "\n",
    "Build a `LinearRegression` model to predict a passenger's age based on the `Pclass`, `SibSp`, `Parch`, `Fare`, `female`, `male`, `S`, `Q`, `C` features. The model should be trained with passengers that have known ages. Use 10-fold cross validation to determine the performance of this model.\n",
    "\n",
    "*Hint - note that for regression models the typical metric of evaluation is the [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error), and that for consistency within the `scikit-learn` API, the [negative mean squared error is returned rather than the mean squared error](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "has_ages = np.where(np.isfinite(titanic_df['Age']))[0]\n",
    "\n",
    "impute_X_train = # complete\n",
    "impute_y_train = # complete\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(impute_X_train)\n",
    "Xminmax = scaler.transform(impute_X_train)\n",
    "\n",
    "lr_age = LinearRegression().fit( # complete\n",
    "\n",
    "cv_results = cross_val_score(LinearRegression(), Xminmax, impute_y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "print('Missing ages have RMSE = {:.2f}'.format(np.mean((-1*cv_results)**0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3n**\n",
    "\n",
    "Use the age regression model to predict the ages for passengers with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "missing_ages = np.where(np.isnan(titanic_df['Age']))[0]\n",
    "\n",
    "impute_X_missing = # complete\n",
    "\n",
    "X_missing_minmax = scaler.transform(impute_X_missing)\n",
    "\n",
    "age_preds = lr_age.predict(X_missing_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3o**\n",
    "\n",
    "Use the imputed age estimates to predict the passenger survival via 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "age_impute = titanic_df['Age'].copy().values\n",
    "age_impute[missing_ages] = # complete\n",
    "\n",
    "titanic_df['age_impute'] = age_impute\n",
    "\n",
    "X = # complete\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "Xminmax = scaler.transform(X)\n",
    "\n",
    "cv_results = cross_val_score( # complete\n",
    "\n",
    "print('The accuracy with the mean for missing ages = {:.2f}%'.format(100*np.mean(cv_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As far as ages are concerned, imputation of the missing data does not significantly improve the model.\n",
    "\n",
    "Which brings us to the concluding lesson in wrangling the Titanic data - not every piece of information is *useful*. This is critical to remember when building machine learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "(As a quick aside - it wouldn't be entirely fair to say there is no useful information in the age feature. It is clear, for example, that \"children,\" i.e. those with Age < 10, had a much higher probability of survival than adults. Perhaps the creation of a `child` feature based on age would improve the model... or using the age in combination with other features, e.g., `Age`x`Pclass` which will further highlight that 1st class passengers were more likely to survive than 3rd class passengers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally - note that you can try to build a model and submit it to Kaggle to see how well you preform on blind data. \n",
    "\n",
    "https://www.kaggle.com/c/titanic - the classifications are not revealed, but from the [leaderboard](https://www.kaggle.com/c/titanic/leaderboard) it is clear that some people were able to build models that perfectly classified the blind data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 4) Wrangling an astro machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we will put together the above examples to work on an astronomical problem of great importance for LSST: photometric redshifts. As we have previously discussed, LSST will observe many, many, many, many more galaxies than we can possibly observe with spectroscopic instruments. As a result, the vast majority of the galaxies observed by LSST will not have precise redshift estimates, and instead we will need to estimate redshifts via photometry alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This week we have heard a lot about `SDSS CasJobs`, and for today's problem we will use 50,000 SDSS sources with spectroscopic measurements to train a machine learning model to estimate redshifts from photometry. The following query was used to generate this training set: \n",
    "\n",
    "    select top 50000 s.specObjID, s.z, s.type,\n",
    "       psfMag_u, psfMag_g, psfMag_r, psfMag_i, psfMag_z, \n",
    "       modelMag_u, modelMag_g, modelMag_r, modelMag_i, modelMag_z,\n",
    "       extinction_u, extinction_g, extinction_r, extinction_i, extinction_z,\n",
    "       w.w1mpro, w1snr, w.w2mpro, w2snr, w.w3mpro, w3snr, w.w4mpro, w4snr\n",
    "       from specphotoall s \n",
    "         join (WISE_xmatch wx  \n",
    "           join WISE_allsky w on wx.wise_cntr = w.cntr) on s.objid = wx.sdss_objid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A few notes about the data: `specObjID` is the SDSS identifier, `z` is the redshift, `type` is the photometric type (\"ps\" for point source or \"ext\" for extended), `psfMag` is the SDSS PSF mag in each filter, `modelMag` is the aperture-matched model mag in each filter, `extinction` is the SFD extinction estimate, `w?mpro` is the [WISE](https://www.nasa.gov/mission_pages/WISE/main/index.html) satellite mid-IR magnitude in the W1, W2, W3, and W4 bands, and `w?msnr` is the SNR in each of the respective filters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*quick aside* - the SDSS photometric type is reported as either 6 or 3, and, for convenience, I have converted these to 'ps' and 'ext' in the file used for the problem below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Important note** - not every SDSS source will have detections in each of the WISE filters. If `w?msnr` < 2, then the `w?mpro` number represents an upper limit, *not* a detection (i.e., these are cases with missing data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4a**\n",
    "\n",
    "Download and read in the [training set](https://northwestern.box.com/s/sjegm0tx62l2i8dkzqw22s4gmq1a9sg1) for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sdss = pd.read_csv(\"DSFP_SDSS_spec_train.csv\")\n",
    "sdss[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4b**\n",
    "\n",
    "Are there categorical features in the dataset? If yes, convert the categorical features for use in machine learning models.\n",
    "\n",
    "*Hint* - recall **Problem 3h** and **3i**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "type_dict = create_bin_cat_feats( # complete\n",
    "\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4c**\n",
    "\n",
    "Is there missing data in the training set? If yes, how will you deal these data? \n",
    "\n",
    "*Hint* - you were already told there is missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# WISE non-detections have SNR < 2\n",
    "\n",
    "for wsnr in ['w1snr', 'w2snr', 'w3snr', 'w4snr']:\n",
    "    frac_missing = # complete\n",
    "    print('{:.2f}% of the obs in {} are non-detections'.format(100*frac_missing, wsnr[0:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given that there are obs missing in each filter, with a substantial number missing in W3 and W4, we will create a new categorical variable for detection in each filter. We will also replace \"upper limits\" with -9.99 (do this in the cell below).\n",
    "\n",
    "(Alternative strategies that could prove worthwhile include: removing W3 and W4 as they are largely missing, imputing the missing values for non-detections, or removing individual sources with non-detections. This last choice would really cut the training set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for filt in ['w1', 'w2', 'w3', 'w4']:\n",
    "    det = np.ones(len(sdss)).astype(int)\n",
    "    det[np.where(sdss['{}snr'.format(filt)] < 2)] = 0\n",
    "    sdss['{}det'.format(filt)] = det\n",
    "    mag = sdss['{}mpro'.format(filt)].values\n",
    "    mag[det == 0] = -9.99\n",
    "    sdss['{}mag'.format(filt)] = mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we have dealt with missing and categorical data we can construct out machine learning model. We will use a $k$ nearest neighbors regression model to determine how well we can measure photometric redshifts.\n",
    "\n",
    "The `sdss` dataframe now includes far more columns than are necessary for the model. Should any of these be excluded from the fitting? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 4d**\n",
    "\n",
    "Build a `scikit-learn` appropriate `X` and `y` array for the features and labels (i.e. redshifts) of the photo-z training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X = # complete\n",
    "y = # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4e**\n",
    "\n",
    "Estimate the RMSE for a [`KNeighborsRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) via 10 fold CV.\n",
    "\n",
    "*Hint* - use [`cross_val_predict`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) in this case so you can plot $z_\\mathrm{SDSS}$ vs. $z_\\mathrm{kNN}$.\n",
    "\n",
    "*Hint 2* - make the plot in a separate cell so you can adjust it without needing to re-run the CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# cross validation goes here\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "knn_reg = # complete\n",
    "y_preds = cross_val_predict( # complete\n",
    "print('The RMSE = {}'.format(np.sqrt(mean_squared_error(y,y_preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plotting goes here\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.scatter( # complete\n",
    "ax.plot([0,6],[0,6], 'Crimson')\n",
    "ax.set_xlabel('z_SDSS')\n",
    "ax.set_ylabel('z_kNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you made a plot of redshift vs. predictions, you likely saw that there are many sources that are far from the 1 to 1 line. These are called catastrophic outliers, and they are a serious problem for science programs that rely on photometric redshifts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 4f**\n",
    "\n",
    "Write a function that takes input arrays `ground_truth` and `predictions` and determines the fraction of the dataset that is catastrophic outliers. For today's purposes we will say a catastrophic outlier is one where the prediction differs by 20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def catastrophic_fraction(ground_truth, predictions, threshold=0.2):\n",
    "    '''Function to calculate fraction of predictions that are catastrophic outliers\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    ground_truth : array-like\n",
    "        Correct labels for the model sources\n",
    "    \n",
    "    predictions : array-like\n",
    "        Predictions for the model sources\n",
    "    \n",
    "    threshold : float (optional, default=0.2)\n",
    "        The threshold to determine if a \"miss\" is catastrophic or not\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    oh_nos : float\n",
    "        Fractional number of catastrophic outliers\n",
    "    '''\n",
    "    # complete\n",
    "    # complete\n",
    "\n",
    "    return oh_nos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4g**\n",
    "\n",
    "How many catastrophic outliers did your previous predictions have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "catastrophic_fraction( # complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Earlier we saw that the performance of a kNN model greatly improves with feature scaling.\n",
    "\n",
    "**Problem 4h**\n",
    "\n",
    "Use a MinMax scaler to scale the features, performs 10 fold cross-validation, and estimate the catastrophic outlier fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "Xminmax = scaler.transform(X)\n",
    "\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=7)\n",
    "y_preds = cross_val_predict( # complete\n",
    "print('The RMSE = {}'.format(np.sqrt(mean_squared_error(y,y_preds))))\n",
    "print('{} are catastrophic'.format(catastrophic_fraction(y, y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plotting goes here\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter( # complete\n",
    "ax.plot([0,6],[0,6], 'Crimson')\n",
    "ax.set_xlabel('z_SDSS')\n",
    "ax.set_ylabel('z_kNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The MinMax scaler didn't help at all!\n",
    "\n",
    "Perhaps we need a different kind of wrangling for this data set - feature engineering. (For example, does it make sense to include `extinction` as a feature, or should we instead combine the extinction with the observed magnitudes to get extinction corrected brightness measurements?)\n",
    "\n",
    "Finally - we close with a machine learning question, not a data wrangling question. Does it actually make sense to use a kNN model for datasets that have categorical features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4i**\n",
    "\n",
    "Build a better machine learning model, possibly with the use of different features, to improve the model classification performance.\n",
    "\n",
    "*Hint* - there was a hint about this in yesterday's lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plotting goes here\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.scatter( # complete\n",
    "ax.plot([0,6],[0,6], 'Crimson')\n",
    "ax.set_xlim(-0.1,4)\n",
    "ax.set_ylim(-0.1,4)\n",
    "ax.set_xlabel('z_SDSS')\n",
    "ax.set_ylabel('z_kNN')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
