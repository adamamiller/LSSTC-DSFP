{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This Laptop Is Inadequate:\n",
    "# An Aperitif for DSFP Session 8\n",
    "\n",
    "**Version 0.1**\n",
    "\n",
    "By AA Miller 2019 Mar 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When I think about LSST there are a few numbers that always stick in my head: \n",
    "\n",
    "  -  37 billion (the total number of sources that will be detected by LSST)\n",
    "  -  10 (the number of years for the baseline survey)\n",
    "  -  1000 (the, approximate, number of observations per source)\n",
    "  -  37 trillion ($37 \\times 10^9 \\times 10^4$ = the total number of source observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These numbers are *eye-popping*, though the truth is that there are now several astronomical databases that have $\\sim{10^9}$ sources (e.g., PanSTARRS-1, which we will hear more about later today). \n",
    "\n",
    "A pressing question, for current and future surveys, is: how are we going to deal with all that data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you're anything like me - then, you love your laptop. \n",
    "\n",
    "And if you had it your way, you wouldn't need anything but your laptop for anything that you have ever worked on.\n",
    "\n",
    "But is that practical?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 1) The Inadequacy of Laptops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 1a**\n",
    "\n",
    "Suppose you could describe every source detected by LSST with a single number. Assuming you are on a computer with a 64 bit architecture, to within an order of magnitude, how much RAM would you need to store every LSST source within your laptop's memory?\n",
    "\n",
    "*Bonus question* - can you think of a single number to describe every source in LSST that could produce a meaningful science result?\n",
    "\n",
    "*Take a minute to discuss with your partner*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\frac{64 \\, \\mathrm{bit}}{1 \\, \\mathrm{source}} \\times \\frac{1 \\,\\mathrm{GB}}{8\\times10^9 \\,\\mathrm{bit}} \\times 3.7 \\times 10^{10}\\, \\mathrm{sources} \\approx 296 \\, \\mathrm{GB}$$\n",
    "\n",
    "While there are specialized machines that have this amount of memory, this is unreasonable for any modern laptops. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As for a single number to perform useful science, I can think of two. \n",
    "\n",
    "First - you could generate a [heirarchical triangular mesh](http://www.skyserver.org/HTM/) with enough trixels to characterize every LSST resolution element on the night sky. Then you could assign a number to each trixel, and describe the position of every source in LSST with a single number. Under the assumption that every source detected by LSST is a galaxy, this is not a terrible assumption, you could look at the clustering of these positions to (potentially) learn things about structure formation or galaxy formation (though without redshifts you may not learn all that much).\n",
    "\n",
    "The other number is the flux (or magnitude) of every source in a single filter. Again, under the assumption that everything is a galaxy, the number counts (i.e. a histogram) of the flux measurements tells you a bit about the Universe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It probably isn't a shock that you won't be able to analyze every individual LSST source on your laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But that raises the question - how should you analyze LSST data?\n",
    "\n",
    "  -  By buying a large desktop?\n",
    "  -  On a local or national supercomputer?\n",
    "  -  In the cloud?\n",
    "  -  On computers that LSST hosts/maintains?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But that raises the question - how should you analyze LSST data?\n",
    "\n",
    "  -  By buying a large desktop? (impractical to ask of everyone working on LSST)\n",
    "  -  On a local supercomputer? (not a bad idea, but not necessarily equitable)\n",
    "  -  In the cloud? (AWS is expensive)\n",
    "  -  On computers that LSST hosts/maintains? (probably the most fair, but this also has challenges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will discuss some of these issues a bit later in the week..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 2) Laptop or Not You Should Be Worried About the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pop quiz\n",
    "\n",
    "We will now re-visit a question from a previous session:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 2a**\n",
    "\n",
    "What is data?\n",
    "\n",
    "*Take a minute to discuss with your partner*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Solution 2a**\n",
    "\n",
    "Data are constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This leads to another question: \n",
    "\n",
    "Q - What is the defining property of a constant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A - They don't change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If data are constants, and constants don't change, then we should probably be sure that our data storage solutions do not alter the data in any way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Within the data science community, the python [`pandas`](https://pandas.pydata.org/) package is particularly popular for reading, writing, and manipulating data (we will talk more about the utility of `pandas` later). \n",
    "\n",
    "The `pandas` docs state the `read_csv()` method is the [workhorse function for reading text files](http://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#csv-text-files). Let's now take a look at how well this workhorse \"maintains the constant nature of data\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2b**\n",
    "\n",
    "Create a `numpy` array, called `nums`, of length 10000 filled with random numbers. Create a `pandas` `Series` object, called `s`, based on that array, and then write the `Series` to a file called `tmp.txt` using the `to_csv()` method.\n",
    "\n",
    "*Hint* - you'll need to name the `Series` and add the `header=True` option to the `to_csv()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nums = np.random.rand(10000)\n",
    "s = pd.Series(nums, name='nums')\n",
    "s.to_csv('tmp.txt', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2c**\n",
    "\n",
    "Using the `pandas` `read_csv()` method, read in the data to a new variable, called `s_read`. Do you expect `s_read` and `nums` to be the same? Check whether or not you expectations are correct. \n",
    "\n",
    "*Hint* - take the sum of the difference not equal to zero to identify if any elements are not the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2302"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_read = pd.read_csv('tmp.txt')\n",
    "\n",
    "sum(nums - s_read['nums'].values != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, it turns out that $\\sim{23}\\%$ of the time, `pandas` does not in fact read in the same number that it wrote to disk.\n",
    "\n",
    "The truth is that these differences are quite small (see next slide), but there are many mathematical operations (e.g., subtraction of very similar numbers) that may lead these tiny differences to compound over time such that your data are not, in fact, constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "print(np.max(np.abs(nums - s_read['nums'].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, what is going on?\n",
    "\n",
    "Sometimes, when you convert a number to ASCII (i.e. text) format, there is some precision that is lost in that conversion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do you avoid this?\n",
    "\n",
    "One way is to directly write your files in binary. To do so has serveral advantages: it is possible to reproduce byte level accuracy, and, binary storage is almost always more efficient than text storage (the same number can be written in binary with less space than in ascii). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The downside is that developing your own procedure to write data in binary is a pain, and it places strong constraints on where and how you can interact with the data once it has been written to disk. \n",
    "\n",
    "Fortuantely, we live in a world with `pandas`. All this hard work has been done for you, as `pandas` naturally interfaces with the `hdf5` binary table format. (You may want to also take a look at `pyTables`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2d**\n",
    "\n",
    "Repeat your procedure from above, but instead of writing to a csv file, use the `pandas` `to_hdf()` and `read_df()` method to see if there are any differences in `s` and `s_read`.  \n",
    "\n",
    "*Hint* - You will need to specify a name for the table that you have written to the `hdf5` file in the call to `to_hdf()` as a required argument. Any string will do.\n",
    "\n",
    "*Hint 2* - Use `s_read.values` instead of `s_read['nums'].values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.to_hdf('tmp.h5', 's', header=True)\n",
    "s_read = pd.read_hdf('tmp.h5', 's')\n",
    "\n",
    "sum(nums - s_read.values != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, if you are using `pandas` anyway, and if you aren't using `pandas` I strongly suggest you make it part of your normal workflow, then I strongly suggest removing csv files from your workflow to instead focus on binary hdf5 files. This requires typing the same number of characters, but it ensures byte level reproducibility. \n",
    "\n",
    "And reproducibiliy is the pillar upon which the scientific method is built. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Is that the end of the story? ... No.\n",
    "\n",
    "In the previous example, I was being a little tricky in order to make a point. It *is* in fact possible to create reproducible csv files with `pandas`. By default, `pandas` sacrifices a little bit of precision in order to gain a lot more speed. If you want to ensure reproducibility then you can specify that the `float_precision` should be `round_trip`, meaning you get the same thing back after reading from a file that you wrote to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.to_csv('tmp.txt', header=True, index=False)\n",
    "\n",
    "s_read = pd.read_csv('tmp.txt', float_precision='round_trip')\n",
    "\n",
    "sum(nums - s_read['nums'].values != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So was all of this in service of a lie?\n",
    "\n",
    "No. What I said before remains true - text files do not guarantee byte level precision, and they take more space on disk. But there are some advantages to using text files: \n",
    "\n",
    "  -  anyone, anywhere, on essentially any platform can easily inspect and deal with text files\n",
    "  -  text files can be easily inspected (and corrected) if necessary\n",
    "  -  special packages are needed to read/write in binary\n",
    "  -  binary files, which are not easily interpretable, are difficult to use in version control (and banned by some version control platforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To summarize, here is my advice: think of binary as your (new?) default for storing data.\n",
    "\n",
    "But, as with all things, consider your audience: if you are sharing/working with people that won't be able to deal with binary data, or, you have an incredibly small amount of data, csv (or other text files) should be fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Problem 3) But what really matters is organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# a = '0.3066101993807095471566981359501369297504425048828125'\n",
    "a = np.pi*1e19\n",
    "print(a)\n",
    "with open('tmp.txt','w') as fw:\n",
    "    print('{:.16f}'.format(a), file=fw)\n",
    "with open('tmp.txt') as f:\n",
    "    b = f.readline()\n",
    "\n",
    "print(float(b) - a)\n",
    "\n",
    "float(b)\n",
    "    \n",
    "    \n",
    "# for i in range(10000):\n",
    "#     with open('tmp.txt') as f:\n",
    "#         ha = f.readline()\n",
    "#     with open('tmp.txt','w') as fw:\n",
    "#         ha = np.float(ha)\n",
    "#         print(ha, file=fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha = np.random.rand(100000)\n",
    "# with open('tmp.txt','w') as fw:\n",
    "#     for h in ha:\n",
    "#         print(h, file=fw)\n",
    "np.savetxt('tmp.txt', ha)\n",
    "b = np.loadtxt('tmp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ha - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141592653589793\n"
     ]
    }
   ],
   "source": [
    "with open('tmp.txt','w') as fw:\n",
    "    print(np.pi)\n",
    "    print(14, file=fw)\n",
    "\n",
    "for i in range(100):\n",
    "    with open('tmp.txt') as f:\n",
    "        ha = f.readline()\n",
    "    with open('tmp.txt','w') as fw:\n",
    "        ha = np.float(ha)\n",
    "        print(ha, file=fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.00000001765"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ha[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
