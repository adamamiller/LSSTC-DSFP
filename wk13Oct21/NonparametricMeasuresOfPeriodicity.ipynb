{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cddf8f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import emcee\n",
    "import george\n",
    "from george import kernels\n",
    "import corner\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34db36ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-parametric Measures of Periodicity â€“ Return of the Gaps\n",
    "\n",
    "**Version 0.1**\n",
    "\n",
    "* * *\n",
    "\n",
    "By AA Miller (Northwester/CIERA)  \n",
    "20 Sep 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca999b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this lecture we will examine non-parametric methods to search for periodic signals in astronomical time series. Lecture III focused extensively on the Lomb-Scargle periodogram. LS is the \"standard\" in astronomy, in part because it was the first (good) method developed for noisy and sparse data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c36a54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "LS is not without warts, however, (i) LS does not handle outliers well, and (ii) LS works best on purely sinusoidal signals.\n",
    "\n",
    "Given non-Gaussian noise and that some signals (e.g., transiting planets) are not sinusoidal, we will now explore alternative methods to search for periodicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b12a7c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We begin today with a simulated signal (we will use multiple harmonics to make things a bit more challenging than a pure sinusoid). The cell below creates a periodic signal with $P = 0.7\\,\\mathrm{d}$, sampled over two months ($60\\,\\mathrm{d}$), with an average of two observations per night."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ca732",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "(If the slide below does not execute properly, jump to the end of this notebook and execute the cells with the various helper functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f59d7da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(185)\n",
    "t_obs = np.random.uniform(60, size=120)\n",
    "phi = np.pi\n",
    "var_y = 0.81\n",
    "\n",
    "y = gen_periodic_data(t_obs, period=0.7, \n",
    "                      amplitude=3, phase=phi, \n",
    "                      noise=var_y) \n",
    "y += gen_periodic_data(t_obs, period=0.7/2, \n",
    "                       amplitude=3, phase=phi+np.pi/2)\n",
    "y += gen_periodic_data(t_obs, period=0.7/3, \n",
    "                       amplitude=2, phase=phi)\n",
    "\n",
    "y_unc = np.ones_like(y)*np.sqrt(var_y)\n",
    "\n",
    "phase_plot(t_obs, y, 0.7, y_unc = y_unc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84827c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For this simulated data, lets run LS and see what we find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138cdf5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from astropy.timeseries import LombScargle\n",
    "\n",
    "freq, psd = LombScargle(t_obs, y, y_unc).autopower(maximum_frequency=5)\n",
    "\n",
    "best_period = 1/freq[np.argmax(psd)]\n",
    "print('Top LS period is {}'.format(best_period))\n",
    "\n",
    "phase_plot(t_obs, y, best_period, y_unc = 0.1*np.ones_like(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d37ffee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "LS only recovers the \"half-period\" as the correct answer! \n",
    "\n",
    "This is a common problem for eclipsing binaries, which are not purely sinusoidal signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11dde3a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "(As you might expect, the input period in the data that was simulated with 3 harmonic terms can be recovered with [`LombScargle`](https://docs.astropy.org/en/stable/api/astropy.timeseries.LombScargle.html) if more than one harmonic is searched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b5914",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we will explore several alternatives to LS, and demonstrate that they can be implemented in `python` without too much overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2bec8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Method 1) String Length\n",
    "\n",
    "The string length method ([Dworetsky](http://adsabs.harvard.edu/abs/1983MNRAS.203..917D)), phase folds the data at trial periods and then minimizes the distance to connect the phase-ordered observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47c2c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img style=\"display: block; margin-left: auto; margin-right: auto\" src=\"./images/StringLength.png\" align=\"middle\">\n",
    "\n",
    "<div align=\"right\"> <font size=\"-3\">(credit: Gaveen Freer - http://slideplayer.com/slide/4212629/#) </font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0382f913",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Probelm 1a**\n",
    "\n",
    "Write a function, `calc_string_length`, that calculates the string length for a phase-folded light curve with observations `x`, `y`, and frequency `f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8cc481",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def calc_string_length(x, y, f=1):\n",
    "    '''Calculate string length for observations at frequency f\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        input time of observations\n",
    "    \n",
    "    y : array-like\n",
    "        measured signal at input x\n",
    "    \n",
    "    f : float (default=1)\n",
    "        frequency of the test period\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sl : float\n",
    "        String length for the phase-ordered observations\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    phases = x*f % 1\n",
    "    sl = # complete\n",
    "    return sl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a224c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 1b** \n",
    "\n",
    "Write a function `sl_periodogram` to measure the string length for input data `x`, `y`, over a frequency grid `f_grid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e024e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def sl_periodogram(x, y, f_grid = np.linspace(0.1,10,10)):\n",
    "    '''Calculate the string length \"periodogram\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        input time of observations\n",
    "    \n",
    "    y : array-like\n",
    "        measured signal at input x\n",
    "    \n",
    "    f_grid : array_like (default=np.linspace(0.1,10,10))\n",
    "        frequency grid for the period\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sl_psd : array_like\n",
    "        String length at every test frequency f\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    sl_psd = np.zeros_like(f_grid)\n",
    "    for f_num, f in enumerate(f_grid):\n",
    "        sl_psd[f_num] = # complete\n",
    "    \n",
    "    return sl_psd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e53ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 1c**\n",
    "\n",
    "Plot the string length periodogram for the simulated data. Does it make sense?\n",
    "\n",
    "*Hint - think about the optimal grid from Notebook III*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30600939",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f_grid = np.arange(1/np.ptp(t_obs), 10, 1/5/np.ptp(t_obs))\n",
    "sl_psd = sl_periodogram( # complete\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(1/f_grid, sl_psd, '0.2', lw=2)\n",
    "ax.set_xlabel('Period (d)')\n",
    "ax.set_ylabel('String Length')\n",
    "\n",
    "ax.axvline(0.7, color='DarkOrange', \n",
    "          lw=1, ls='--')\n",
    "\n",
    "axins = plt.axes([.29, .22, .65, .27])\n",
    "axins.plot(1/f_grid, sl_psd, '0.2', lw=2)\n",
    "axins.axvline(0.7, color='DarkOrange',  lw=1, ls='--')\n",
    "axins.set_xlim(0,3)\n",
    "\n",
    "fig.subplots_adjust(left=0.1, right=0.99, top=0.98, bottom=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad52e709",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The string length method was able to recover the correct period for the simulated data. \n",
    "\n",
    "The main downside to this method is that it does not account for the observational uncertainties at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6910c0ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Method 2) Phase Dispersion Minimization\n",
    "\n",
    "Phase Dispersion Minimization (PDM; [Jurkevich 1971](http://adsabs.harvard.edu/abs/1971Ap%26SS..13..154J), [Stellingwerth 1978](http://adsabs.harvard.edu/abs/1978ApJ...224..953S)), like LS, folds the data at a large number of trial frequencies $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d27225",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The phased data are then binned, and the variance is calculated in each bin, combined, and compared to the overall variance of the signal. No functional form of the signal is assumed, and thus, non-sinusoidal signals can be found.\n",
    "\n",
    "<img style=\"display: block; margin-left: auto; margin-right: auto\" src=\"./images/PDM.jpg\" align=\"middle\">\n",
    "\n",
    "<div align=\"right\"> <font size=\"-3\">(credit: Gaveen Freer - http://slideplayer.com/slide/4212629/#) </font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368a46b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2a** \n",
    "\n",
    "Write a function called `calc_pdm`, that calculates the average dispersion/scatter in $N$ equally spaced bins for a phase-folded light curve with observations `x`, `y`, and frequency `f`. Specify the number of bins $N$ via keyword argument `bins`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b8fb6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def calc_pdm(x, y, f=1, bins=10):\n",
    "    '''Calculate the phase dispersion minimization for observations at frequency f\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        input time of observations\n",
    "    \n",
    "    y : array-like\n",
    "        measured signal at input x\n",
    "    \n",
    "    f : float (default=1)\n",
    "        frequency of the test period\n",
    "    \n",
    "    bins : int (default=10)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pdm : float\n",
    "        the sum of the scatter in each bin\n",
    "    '''\n",
    "    phases = x*f % 1\n",
    "    # complete\n",
    "    # complete    \n",
    "    # complete\n",
    "    # complete\n",
    "\n",
    "    return pdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f52349e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2b** \n",
    "\n",
    "Write a function `pdm_periodogram` to measure the relative reduction in the scatter for a phase-folded light curve with input data `x`, `y`, over a frequency grid `f_grid`.\n",
    "\n",
    "Plot the periodogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632b6cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def pdm_periodogram(x, y, f_grid = np.linspace(0.1,10,10), **kwargs):\n",
    "    '''Calculate the phase dispersion minimization \"periodogram\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        input time of observations\n",
    "    \n",
    "    y : array-like\n",
    "        measured signal at input x\n",
    "    \n",
    "    f_grid : array_like (default=np.linspace(0.1,10,10))\n",
    "        frequency grid for the period\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pdm_psd : array_like\n",
    "        PDM at every test frequency f\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    pdm_psd = np.zeros_like(f_grid)\n",
    "    total_rms = np.std(y, ddof=1)\n",
    "    for f_num, f in enumerate(f_grid):\n",
    "        pdm_psd[f_num] = calc_pdm( # complete\n",
    "    \n",
    "    return pdm_psd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a5eea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f_grid = np.arange(1/np.ptp(t_obs), 10, 1/5/np.ptp(t_obs))\n",
    "pdm_psd = pdm_periodogram( # complete\n",
    "\n",
    "print(f'The best-fit period is {1/f_grid[np.argmin(pdm_psd)]:.4f} d')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(1/f_grid, pdm_psd, '0.2', lw=2)\n",
    "ax.set_xlabel('Period (d)')\n",
    "ax.set_ylabel('PDM statistic')\n",
    "ax.axline((0.7,np.mean(pdm_psd)), (0.7, np.mean(pdm_psd)+1e-3), \n",
    "          color='DarkOrange',  lw=1, ls='--')\n",
    "\n",
    "\n",
    "axins = plt.axes([.29, .22, .65, .27])\n",
    "axins.plot(1/f_grid, pdm_psd, '0.2', lw=2)\n",
    "axins.axline((0.7,np.mean(pdm_psd)), (0.7, np.mean(pdm_psd)+1e-3), \n",
    "          color='DarkOrange',  lw=1, ls='--')\n",
    "axins.set_xlim(0,3)\n",
    "fig.subplots_adjust(left=0.1, right=0.99, top=0.98, bottom=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58b5b4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PDM finds the correct period! \n",
    "\n",
    "Like string length, PDM does not incorporate observational uncertainties, though a slight modification to measure the $\\chi^2$ rather than the scatter can correct that. \n",
    "\n",
    "The main challenge for PDM is deciding the number of bins to adopt. For some light curves the choice of 10 or 100 bins can result in different measurements of the best period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b48e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Method 3) Analysis of Variance\n",
    "\n",
    "Analysis of Variance (AOV; [Schwarzenberg-Czerny 1989](http://adsabs.harvard.edu/abs/1989MNRAS.241..153S)) is similar to PDM. Optimal periods are defined via hypothesis testing, and these methods are found to perform best for certain types of astronomical signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e9852",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Method 4) Supersmoother\n",
    "\n",
    "Supersmoother ([Reimann](http://adsabs.harvard.edu/abs/1994PhDT........20R)) is a least-squares approach wherein a flexible, non-parametric model is fit to the folded observations at many trial frequncies. The use of this flexible model reduces aliasing issues relative to models that assume a sinusoidal shape, however, this comes at the cost of requiring considerable computational time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ddd4c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Briefly, supersmoother provides a smooth estimate of the data via localized linear regression. Observations are then compared to the smooth model value to identify the model that optimally reduces the sum of the square of the residuals (normalized by the uncertainties when available). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7942fbd2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Supersmoother requires a user-selected smoothing window (called the \"span\"), a sliding region over which the linear fit is performed. \n",
    "\n",
    "The \"magic\" in supersmoother is that it uses cross-validation to identify the optimal span at every location within the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fa9a37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The supersmoother psuedo-code is:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170abbc",
   "metadata": {},
   "source": [
    " 1. create 3 smooth local linear estimations of `y` at every input `x` with `span` = 0.05, 0.2, and 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c73f8d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   2. identify optimal `span` at every `x` based on residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2dbbf9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  3. smooth the above \"optimal\" span curve with `span` = 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdfea06",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  4. create a \"final\" smooth estimate by interpolating bewtween two smooth curves closest in value to (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2958758",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will now illustrate how this works via several examples (before putting everything together into a single function). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b22812",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4a**\n",
    "\n",
    "Write a function `smooth` that estimates the value of `y` at every phase `phase` via a linear least squares fit to all the observations within $\\pm$`span`/2 of `phase`. The observed value of `y` at phase `phase` should be excluded from the fit. \n",
    "\n",
    "*Hint* - it may be helpful to input `x` and `f` so the phase can be calculated within the function. Note - you can \"supersmooth\" any series of data, a frequency is not strictly required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368d51f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def smooth(y, x, f=None, span=0.05, y_unc=None):\n",
    "    '''Calculate the smooth\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        input time of observations\n",
    "    \n",
    "    y : array-like\n",
    "        measured signal at input x\n",
    "    \n",
    "    f : float (optional; default=None)\n",
    "        frequency for which to calculate the smooth.\n",
    "        if None, then the x values are normalized \n",
    "        between 0 and 1.\n",
    "\n",
    "    y_unc : array-like (optional; default=None)\n",
    "        uncertainties on the input signal\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    smooth : array_like\n",
    "        smooth estimate of the phase folded frequency\n",
    "    '''\n",
    "    \n",
    "    if type(y_unc) == int:\n",
    "        y_unc = np.ones_like(y)*y_unc\n",
    "        \n",
    "    if f is None:\n",
    "        phases = (x - np.min(x))/(np.ptp(x))\n",
    "    else:\n",
    "        phases = (x*f) % 1\n",
    "        \n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "        \n",
    "    return smooth\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed79f46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4b**\n",
    "\n",
    "Plot the smooth representation of the data with spans of 0.05, 0.2, and 0.5 folded at a period of 0.7 d. \n",
    "\n",
    "*pseudocode 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e13ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "phases = (t_obs/0.7) % 1\n",
    "\n",
    "smooth_tweeter = smooth(y, t_obs, 1/0.7, span=0.05, y_unc=y_unc)\n",
    "smooth_midrange = smooth(y, t_obs, 1/0.7, span=0.2, y_unc=y_unc)\n",
    "smooth_woofer = smooth(y, t_obs, 1/0.7, span=0.5, y_unc=y_unc)\n",
    "\n",
    "phase_plot(t_obs, y, 0.7, y_unc = 0.1*np.ones_like(y))\n",
    "plt.plot(np.sort(phases), smooth_tweeter[np.argsort(phases)], \n",
    "       label=\"span = 0.05\")\n",
    "plt.plot(np.sort(phases), smooth_midrange[np.argsort(phases)], \n",
    "       label=\"span = 0.2\")\n",
    "plt.plot(np.sort(phases), smooth_woofer[np.argsort(phases)], \n",
    "       label=\"span = 0.5\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3abcb5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now have 3 different \"smooth\" representations of the data. But even these \"smooth\" representations are a bit jagged, and in places we can see the influence of noise.\n",
    "\n",
    "We will now identify the \"optimal\" span at every phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032f22b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Probem 4c**\n",
    "\n",
    "Identify the best span at every phase via the residuals. \n",
    "\n",
    "*pseudocode 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1db97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "smooth_list = np.vstack([[smooth_tweeter], \n",
    "                         [smooth_midrange],\n",
    "                         [smooth_woofer]])\n",
    "span_list = np.vstack([[np.ones_like(smooth_tweeter)*0.05], \n",
    "               [np.ones_like(smooth_midrange)*0.2],\n",
    "               [np.ones_like(smooth_woofer)*0.5]])\n",
    "resid = np.abs(y - smooth_list)\n",
    "\n",
    "best_span = span_list[np.argmin(resid, axis=0), 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4facd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4d**\n",
    "\n",
    "Smooth the `best_span` array using a `span = 0.2`, to create a smooth representation of the best smooth.\n",
    "\n",
    "*pseudocode 3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ae7b1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "span_midrange = smooth(best_span, t_obs, 1/0.7, span=0.2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.sort(phases), span_midrange[np.argsort(phases)])\n",
    "ax.set_xlabel('phase')\n",
    "ax.set_ylabel('optimal smooth')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed79fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4e**\n",
    "\n",
    "Calculate the \"super\" smooth representation of the data by interpolating from the 3 initial smooth estimates to the `span_midrange` estimate.\n",
    "\n",
    "*pseudocode 4*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a847b29b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "supersmooth = np.empty_like(smooth_midrange)\n",
    "for sm_num, sm in enumerate(span_midrange):\n",
    "    supersmooth[sm_num] = np.interp(sm, \n",
    "                                    span_list.T[sm_num], \n",
    "                                    smooth_list.T[sm_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737d9d8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4f**\n",
    "\n",
    "Plot the supersmooth over the phase-folded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503bb10b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "phases = (t_obs/0.7) % 1\n",
    "\n",
    "phase_plot(t_obs, y, 0.7, y_unc = 0.1*np.ones_like(y))\n",
    "plt.plot(np.sort(phases), smooth_tweeter[np.argsort(phases)], \n",
    "       label=\"span = 0.05\")\n",
    "plt.plot(np.sort(phases), smooth_midrange[np.argsort(phases)], \n",
    "       label=\"span = 0.2\")\n",
    "plt.plot(np.sort(phases), smooth_woofer[np.argsort(phases)], \n",
    "       label=\"span = 0.5\")\n",
    "plt.plot(np.sort(phases), supersmooth[np.argsort(phases)], \n",
    "         lw=4, color='0.3', zorder=10, \n",
    "         label='supersmooth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f9bf82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4g**\n",
    "\n",
    "Wrap everything from above into a single function `calc_supersmooth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07bb7ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def calc_supersmooth(y, x, f=None, spans=[0.05, 0.2, 0.5], y_unc=None):\n",
    "    '''Calculate the smooth\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        input time of observations\n",
    "    \n",
    "    y : array-like\n",
    "        measured signal at input x\n",
    "    \n",
    "    f : float (default=None)\n",
    "        frequency for which to calculate the smooth.\n",
    "        if None, then the x values are normalized \n",
    "        between 0 and 1.\n",
    "\n",
    "    spans : list (default=[0.05, 0.2, 0.5])\n",
    "        list of the individual spans to use for the \n",
    "        initial smooth representations of the data\n",
    "\n",
    "    y_unc : array-like (default=None)\n",
    "        uncertainties on the input signal\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    supersmooth : array_like\n",
    "        smooth estimate of the phase folded frequency\n",
    "    '''\n",
    "    if type(y_unc) == int:\n",
    "        y_unc = np.ones_like(y)*y_unc\n",
    "        \n",
    "    if f is None:\n",
    "        phases = (x - np.min(x))/(np.ptp(x))\n",
    "    else:\n",
    "        phases = (x*f) % 1\n",
    "\n",
    "    smooth_list = np.vstack([[smooth(y, x, f, span=s, y_unc=y_unc) for s in spans]])\n",
    "    span_list = np.ones_like(smooth_list)*np.array(spans)[:,None]\n",
    "    \n",
    "    resid = np.abs(y - smooth_list)\n",
    "    \n",
    "    span_midrange = smooth(span_list[np.argmin(resid, axis=0), 0], \n",
    "                           x, f, span=np.median(spans))\n",
    "    \n",
    "    supersmooth = np.empty_like(smooth_midrange)\n",
    "    for sm_num, sm in enumerate(span_midrange):\n",
    "        supersmooth[sm_num] = np.interp(sm, \n",
    "                                        span_list.T[sm_num], \n",
    "                                        smooth_list.T[sm_num])\n",
    "    \n",
    "    return supersmooth\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c9a38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4h**\n",
    "\n",
    "Write a function `supersmooth_periodogram` to calculate the Supersmoother periodogram. \n",
    "\n",
    "Unlike the other methods above, we can in this case calculate $\\chi^2$ at each frequency. Use $\\chi^2_0$ as a relative baseline as we did in calculating the LS periodogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02e0d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def supersmooth_periodogram(y, y_unc, x, f_grid):\n",
    "    psd = np.empty_like(f_grid)\n",
    "    chi2_0 = np.sum(((y - np.mean(y))/y_unc)**2)\n",
    "    \n",
    "    for f_num, f in enumerate(f_grid):\n",
    "        supersmooth = calc_supersmooth(y, x, f, y_unc=y_unc)\n",
    "        chi2 = np.sum((y - supersmooth)**2/y_unc**2)\n",
    "        psd[f_num] = 0.5*(chi2_0 - chi2)\n",
    "    \n",
    "    return psd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1fa43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4i**\n",
    "\n",
    "Calculate and plot the supersmoother periodogram for the simulated data. \n",
    "\n",
    "*Hint* - this is much slower than above, use a frequency grid that goes from 0.6 to 3 and has 1000 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162dcd8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f_grid = np.linspace(0.6, 3, 1000)\n",
    "\n",
    "ss_psd = supersmooth_periodogram(y, y_unc, t_obs, f_grid)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(1/f_grid, ss_psd)\n",
    "ax.axvline(0.7, color='DarkOrange', \n",
    "          lw=1, ls='--')\n",
    "ax.set_xlabel('Period (d)')\n",
    "ax.set_ylabel('Power')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567c4a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The grand downside of supersmoother should now be obvious - this thing is S - L - O - W, slow. \n",
    "\n",
    "We only ran a frequency grid with only 1000 points and it still took significantly longer than the other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc7de4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fortunately, Jake VanderPlas has created a faster implementation of [SuperSmoother](https://www.astroml.org/gatspy/periodic/supersmoother.html), if you are interested in implementing this method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbceb20d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have just now covered 4 (4!) alternatives to LS for measuring periodicity (and Notebook IV involves yet another). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1c026",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is something that should really be bothering you at this point though..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774135fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Absolutely none of the methods we have covered provide any sort of measurement of uncertainty on the best-fit period estimates. And uncertainty is at the heart of all meaningful analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087d1cf8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Methods\n",
    "\n",
    "There have been some efforts to frame the period-finding problem in a Bayesian framework. [Bretthorst 1988](https://www.springer.com/us/book/9780387968711) developed Bayesian generalized LS models, while [Gregory & Loredo 1992](http://adsabs.harvard.edu/abs/1992ApJ...398..146G) applied Bayesian techniques to phase-binned models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743f3c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More recently, efforts to use Gaussian processes (GPs) to model and extract a period from the light curve have been developed ([Wang et al. 2012](http://adsabs.harvard.edu/abs/2012ApJ...756...67W)). These methods have proved to be especially useful for detecting stellar rotation in Kepler light curves ([Angus et al. 2018](http://adsabs.harvard.edu/abs/2018MNRAS.474.2094A)). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38e321",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Method 5) GPs + the Quasi-Periodic Kernel\n",
    "\n",
    "Our desired goal is to get some form of probabilistic estimate on the period. This can be done using a GP plus some Bayesian inference techniques, as the references above demonstrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d41159f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "(I realize that for half of you we have not yet had a chance to cover Bayesian statistics. I'm going to provide a woefully short intro/review here, but I'm otherwise writing things in a way that I hope is general enough to see the utility of the following method)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6826d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is my ridiculously short review/intro to Bayesian analysis for this problem: the posterior $P(\\theta|x)$ (i.e., the probability distribution for the model parameters) is proportional to the likelihood multiplied by the prior:\n",
    "\n",
    "$$P(\\theta|x) \\propto P(x|\\theta)P(\\theta)$$\n",
    "\n",
    "where $P(x|\\theta)$ is the likelihood, $P(\\theta)$ is the prior, $\\theta$ is a vector of all the model parameters, and $x$ is the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df8074",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In many applications, including this one, the likelihood $\\mathcal L$, specifically the $\\ln \\mathcal{L}$, is essentially the familiar form of what we often call $\\chi^2$. There can be important implications for the prior but for this example we will find that the prior does not significantly the final probability densities for the model parameters (in part because we have really good data for constraining the period of this particulat eclipsing binary). Thus, we will adopt \"wide and flat\" priors, which are adopted by many, but I'll warn not always the absolute best idea for analysis like this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc04ff6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will adopt a \"standard\" gaussian likelihood (which reduces to the $\\chi^2$ when we calculate the log of the likelihood). \n",
    "\n",
    "All we need now is a model for the signal at every time, $t$, or position, $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b299d24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will model the data as a Gaussian process (GP). In this case we know the signal is periodic (we have an EB). \n",
    "\n",
    "For periodic signals it is common to adopt a cosine kernel for the GP covariance function. As we saw in Notebook III, this particular EB does not have a purely sinusoidal signal (due to the difference in the depths of the two eclipses). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3887293",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Instead, we will adopt the quasi-periodic kernel: \n",
    "\n",
    "$$K_{ij} = k(x_i - x_j) = \\exp \\left(-\\Gamma \\sin^2\\left[\\frac{\\pi}{P} \\left|x_i - x_j\\right|\\right]\\right)$$\n",
    "\n",
    "which is extremely useful for periodic (or quasi-periodic) data with non-sinusoidal signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c340a73e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 5a**\n",
    "\n",
    "Load the light curve from the example EB from Notebook III, and plot the phase folded light curve at the previously identified \"optimal\" period $0.735085\\,\\mathrm{d}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9656e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lc = pd.read_csv(\"example_asas_lc.dat\")\n",
    "\n",
    "phase_plot(lc['hjd'], lc['mag'], 0.735085, lc['mag_unc'], \n",
    "           mag_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36a82e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Evaluating the quasi-periodic (really any) GP kernel when the number of observations is large is computationally expensive. For this purpose we will use [`george`](https://github.com/dfm/george), which performs the necessary matrix algebra quickly. \n",
    "\n",
    "We do not have time for a true introduction to `george`, but I encourage you to [read the docs](https://george.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8cb91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For the MCMC sampling we will use [`emcee`](https://emcee.readthedocs.io/en/stable/index.html), again there is not time for a full introduction. This software will be covered in greater detail elsewhere in the DSFP. \n",
    "\n",
    "It's advantage for our present purposes is that it is written in pure python, and can accept any user defined function for the posterior. It also integrates very nicely with `george`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c706774d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In brief, `emcee` uses multiple MCMC chains that simultaneously explore the posterior probability. During each step within the chain, individual chains (called \"walkers\") will effectively query the other walkers in order to identify the optimal next step in the chain. \n",
    "\n",
    "Multiple chains enables \"easy\" parallelization, though we will not focus on that today as `george` is already highly optimized to use mutliple CPU cores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a9ecb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 5b**\n",
    "\n",
    "Write a function `model` that returns the \"mean model\" for the GP. The two arguments for this function should be a tuple `theta` of length 4, where the 3rd element is the \"mean\" `b`, and the time of observation `t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f1653",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def model(theta, t):\n",
    "    _, _, b, _ = theta\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414256a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 5c**\n",
    "\n",
    "Write a function `lnlike` to calculate the log likelihood for the data given the model parameters `theta`. `theta` should include the log of the period, the log of the amplitude of the signal, the GP mean value `b`, and the log of $\\Gamma$.\n",
    "\n",
    "*Hint* - execute cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f6545",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\log$ in all instances here and below refers to the natural logarithm, or $\\log$ base $\\mathcal{e}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6416c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def lnlike(theta, t, y, yerr):\n",
    "    lnper, lna, b, lngamma = theta\n",
    "    gp = george.GP(np.exp(lna) * \n",
    "                   kernels.ExpSine2Kernel(np.exp(lngamma), lnper))\n",
    "    gp.compute(t, yerr)\n",
    "    return gp.lnlikelihood(y - model(theta, t), quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ba33b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 5d**\n",
    "\n",
    "Write a function `lnprior` to calculate the log of the prior on `theta`. Use a wide and flat prior on every parameter. For numerical reasons, the function should return `-np.inf` if the prior probability is equal to zero.\n",
    "\n",
    "*Hint* - execute the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559021db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def lnprior(theta):\n",
    "    lnper, lna, b, lngamma = theta\n",
    "    if (-20 < lna < 20 and \n",
    "        -20 < b < 20 and \n",
    "        -20 < lngamma < 20 and\n",
    "        -10 < lnper < np.log(10)):\n",
    "        return 0.0\n",
    "    return -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b53b67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 5e**\n",
    "\n",
    "Write a function `lnprob` to calculate the log of the product of the likelihood with the prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6552b1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def lnprob(p, x, y, yerr):\n",
    "    lp = lnprior(p)\n",
    "    return lp + lnlike(p, x, y, yerr) if np.isfinite(lp) else -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d23eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need a starting position for our MCMC chains/walkers. The use of a quasi-periodic kernel means the posterior is highly non-linear, so we cannot expect to start the walkers anywhere and still achieve reasonable results. \n",
    "\n",
    "In this case we will use a little common sense, plus a little computational \"brute force\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c8727",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 5f**\n",
    "\n",
    "Run `LombScarge` on the data and determine the top three peaks in the periodogram. Set `nterms` = 2, and the maximum frequency to 5 (this is arbitrary but sufficient in this case, since the period is > 0.2 d).\n",
    "\n",
    "*Hint* - you may need to search more than the top 3 periodogram values to find the 3 peaks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb0c93",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In [Angus et al. 2018](http://adsabs.harvard.edu/abs/2018MNRAS.474.2094A) a far more sophisticated approach is used for initializing the walkers using the autocorrelation function and adjusting the prior on $P$ - if you want to use GPs to infer periods for real observations I cannot recommend that source enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7f953",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "frequency, power = LombScargle(lc['hjd'], lc['mag'], lc['mag_unc'], nterms=2).autopower(maximum_frequency=5)\n",
    "\n",
    "print('Top LS period is {}'.format(1/frequency[np.argmax(power)]))\n",
    "print(1/frequency[np.argsort(power)[::-1][0:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d831cca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 5g**\n",
    "\n",
    "Initialize one third of your 150 walkers around each of the periods identified in the previous problem. \n",
    "\n",
    "Run the MCMC for 500 steps following this initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aec449",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "initial1 = np.array([np.log(0.735), 1, 10, 1])\n",
    "ndim = len(initial1)\n",
    "nwalkers = 50\n",
    "p1 = [np.array(initial1) + 1e-4 * np.random.randn(ndim)\n",
    "      for i in range(nwalkers)]\n",
    "\n",
    "initial2 = np.array([np.log(0.367), 1, 10, 1])\n",
    "ndim = len(initial2)\n",
    "nwalkers = 50\n",
    "p2 = [np.array(initial2) + 1e-4 * np.random.randn(ndim)\n",
    "      for i in range(nwalkers)]\n",
    "\n",
    "initial3 = np.array([np.log(0.211), 1, 10, 1])\n",
    "ndim = len(initial3)\n",
    "nwalkers = 50\n",
    "p3 = [np.array(initial3) + 1e-4 * np.random.randn(ndim)\n",
    "      for i in range(nwalkers)]\n",
    "p0 = p1+p2+p3\n",
    "\n",
    "nwalkers = len(p0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c18b9e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, \n",
    "                                args=(lc['hjd'],lc['mag'],lc['mag_unc']))\n",
    "for sample in sampler.sample(p0, iterations=500, progress=True):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f25b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 5h**\n",
    "\n",
    "Plot the chains using the `plotChains()` helper function from the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2151d8d",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "paramsNames = ['ln(P)', 'ln(a)', 'b', '$ln(\\gamma)$']\n",
    "nburn = 350\n",
    "plotChains(sampler, nburn, paramsNames)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfb84a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 5i** \n",
    "\n",
    "Plot $\\ln P$ vs. log posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356935fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "chain_lnp_end = sampler.chain[:,-1,0]\n",
    "chain_lnprob_end = sampler.lnprobability[:,-1]\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(chain_lnp_end, chain_lnprob_end, alpha=0.1)\n",
    "ax.set_xlabel('ln(P)')\n",
    "ax.set_ylabel('ln(Probability)')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cbdd9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 5j**\n",
    "\n",
    "Now we will start a new set of MCMC chains all initialized around a \"random ball\" centered on the maximum a posteriori value from the previous set of simulations.$^\\dagger$\n",
    "\n",
    "Run a new MCMC with 150 walkers for 500 steps. \n",
    "\n",
    "Plot the chains. Have they converged?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2554b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$^\\dagger$It is important to remember there are no rules about where an MCMC chain is initialized. We can do whatever we want. I also caution to be mindful of this in future research endeavors, because there is also no guarantee that a finite MCMC chain has identified a global maximum for the posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ad11bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p = p0[np.argmax(chain_lnprob_end)]\n",
    "sampler.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998204a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p0 = [p + 1e-8 * np.random.randn(ndim) for i in range(nwalkers)]\n",
    "sampler.reset() # do not continue the existing chains\n",
    "for sample in sampler.sample(p0, iterations=500, progress=True):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6725fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "paramsNames = ['ln(P)', 'ln(a)', 'b', '$ln(\\gamma)$']\n",
    "nburn = 250\n",
    "plotChains(sampler, nburn, paramsNames)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9231cf3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 5k**\n",
    "\n",
    "Make a corner plot of the samples. What is the marginalized estimate$^\\dagger$ for the period of this source? \n",
    "\n",
    "How does this estimate compare to LS?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44f816",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given that we ultimately used wide and flat priors, which effectively played no role in the analysis of this problem, the discerning viewer might ask \"Why did we bother to use Bayesian analysis at all?\"\n",
    "\n",
    "This problem answers that question â€“ the data cannot be \"marginalized\" (i.e., we ignore everything but the final distribution on the period $P$) in any context except a Bayesian one. Ultimately, marginalization requires an integral where we \"integrate out\" our ignorance of the model parameters that we do not care about. These sorts of integrals require a Bayesian view of probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda15e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "samples = sampler.chain[:, nburn:, :].reshape((-1, ndim))\n",
    "plot_samples = samples.copy()\n",
    "plot_samples[:,0] = np.exp(samples[:,0])\n",
    "fig = corner.corner(plot_samples, \n",
    "                    labels=paramsNames, \n",
    "                    quantiles=[0.16,0.50,0.84])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3fa730",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p16, p50, p84 = np.percentile(samples[:,0], [16,50,84])\n",
    "\n",
    "print('ln(P) = {:.6f} +{:.6f} -{:.6f}'.format(p50, p84-p50, p50-p16))\n",
    "\n",
    "print('GP Period = {:.6f}'.format(np.exp(p50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f253c0c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The cell below shows marginalized samples overplotted on the actual data. How well does the model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b7d2f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.errorbar(lc['hjd'], lc['mag'], lc['mag_unc'], fmt='o')\n",
    "ax.set_xlabel('HJD (d)')\n",
    "ax.set_ylabel('mag')\n",
    "\n",
    "hjd_grid = np.linspace(4790, 4850,5000)\n",
    "\n",
    "for s in samples[np.random.randint(len(samples), size=5)]:\n",
    "    # Set up the GP for this sample.\n",
    "    lnper, lna, b, lngamma = s\n",
    "    gp = george.GP(np.exp(lna) * \n",
    "                   kernels.ExpSine2Kernel(np.exp(lngamma), lnper))\n",
    "    gp.compute(lc['hjd'], lc['mag_unc'])\n",
    "    # Compute the prediction conditioned on the observations and plot it.\n",
    "    m = gp.sample_conditional(lc['mag'] - model(s, lc['hjd']), hjd_grid) + model(s, hjd_grid)\n",
    "    \n",
    "    ax.plot(hjd_grid, m, color=\"0.2\", alpha=0.3)\n",
    "ax.set_xlim(4803, 4832)\n",
    "ax.set_ylim(11.35, 10.8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a9d67",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now you have the tools to fit a GP to a light curve and get an estimate of the best fit period (and to get an estimate of the uncertainty on that period to boot!). \n",
    "\n",
    "As previously noted, you should be a bit worried about \"burn in\" and how the walkers were initialized throughout. If you plan to use GPs to search for periods in your own work, I highly recommend you read [Angus et al. 2018](http://adsabs.harvard.edu/abs/2018MNRAS.474.2094A) on the GP periodogram. Angus et al. provide far more intelligent methods for initializing the MCMC than what is presented here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae873f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Helper Functions\n",
    "\n",
    "We developed useful helper functions as part of [Lecture III](https://github.com/LSSTC-DSFP/LSSTC-DSFP-Sessions/tree/main/Session13/Day3) from this session. \n",
    "\n",
    "These functions generate periodic data, and phase fold light curves on a specified period. These functions will once again prove useful, so we include them here in order to simulate data above. \n",
    "\n",
    "We also add a helper function that can visualize the invidual MCMC chains that are produced by the [`emcee`](https://emcee.readthedocs.io/en/stable/) MCMC sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252dc9a0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Helper 1**\n",
    "\n",
    "Create a function, `gen_periodic_data`, that creates simulated data (including noise) over a grid of user supplied positions:\n",
    "\n",
    "$$ y = A\\,cos\\left(\\frac{x}{P} - \\phi\\right) + \\sigma_y$$\n",
    "\n",
    "where $A, P, \\phi$ are inputs to the function. `gen_periodic_data` should include Gaussian noise, $\\sigma_y$, for each output $y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f689592",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def gen_periodic_data(x, period=1, amplitude=1, phase=0, noise=0):\n",
    "    '''Generate periodic data given the function inputs\n",
    "    \n",
    "    y = A*cos(x/p - phase) + noise\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        input values to evaluate the array\n",
    "    \n",
    "    period : float (default=1)\n",
    "        period of the periodic signal\n",
    "    \n",
    "    amplitude : float (default=1)\n",
    "        amplitude of the periodic signal\n",
    "    \n",
    "    phase : float (default=0)\n",
    "        phase offset of the periodic signal\n",
    "    \n",
    "    noise : float (default=0)\n",
    "        variance of the noise term added to the periodic signal\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : array-like\n",
    "        Periodic signal evaluated at all points x\n",
    "    '''\n",
    "    \n",
    "    y = amplitude*np.sin(2*np.pi*x/(period) - phase) + np.random.normal(0, np.sqrt(noise), size=len(x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18086ce3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Helper 2**\n",
    "\n",
    "Create a function, `phase_plot`, that takes x, y, and $P$ as inputs to create a phase-folded light curve (i.e., plot the data at their respective phase values given the period $P$).\n",
    "\n",
    "Include an optional argument, `y_unc`, to include uncertainties on the `y` values, when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378dbe84",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def phase_plot(x, y, period, y_unc = 0.0, mag_plot=False):\n",
    "    '''Create phase-folded plot of input data x, y\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        data values along abscissa\n",
    "\n",
    "    y : array-like\n",
    "        data values along ordinate\n",
    "\n",
    "    period : float\n",
    "        period to fold the data\n",
    "        \n",
    "    y_unc : array-like\n",
    "        uncertainty of the \n",
    "    '''    \n",
    "    phases = (x/period) % 1\n",
    "    if type(y_unc) == float:\n",
    "        y_unc = np.zeros_like(x)\n",
    "        \n",
    "    plot_order = np.argsort(phases)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.errorbar(phases[plot_order], y[plot_order], y_unc[plot_order],\n",
    "                 fmt='o', mec=\"0.2\", mew=0.1)\n",
    "    ax.set_xlabel(\"phase\")\n",
    "    ax.set_ylabel(\"signal\")\n",
    "    if mag_plot:\n",
    "        ax.set_ylim(ax.get_ylim()[::-1])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f7519",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Helper 3**\n",
    "\n",
    "Write a function `plot_chains` to show the individual chains from the multi-chain MCMC sampler `emcee`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30f140",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#define function to plot walker chains  \n",
    "def plotChains(sampler, nburn, paramsNames, nplot=None):\n",
    "    '''Plot individual chains from the emcee MCMC sampler\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sampler : emcee EnsembleSampler object\n",
    "        emcee affine-invariant multi-chain MCMC sampler\n",
    "    \n",
    "    nburn : int\n",
    "        number of \"burn-in\" steps for the MCMC chains\n",
    "    \n",
    "    paramsNames : list\n",
    "        names of the parameters to be shown\n",
    "    \n",
    "    nplot : int (default=None)\n",
    "        number of chains to show in the visualization.\n",
    "        In instances where the number of chains is \n",
    "        very large (>> 100), then it can be helpful to \n",
    "        downsample to provide more clarity.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ax : maptlotlib axes object\n",
    "        multi panel plot showing the evoltion of \n",
    "        each chain for the parameters in the model\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Nparams = len(paramsNames)\n",
    "    nwalkers = sampler.get_chain().shape[1]\n",
    "    \n",
    "    fig, ax = plt.subplots(Nparams+1,1, figsize = (8,2*(Nparams+1)), sharex = True)\n",
    "    fig.subplots_adjust(hspace = 0)\n",
    "    ax[0].set_title('Chains')\n",
    "    xplot = np.arange(sampler.get_chain().shape[0])\n",
    "\n",
    "    if nplot is None:\n",
    "        nplot=nwalkers\n",
    "    selected_walkers = np.random.choice(range(nwalkers), nplot, replace=False)\n",
    "    for i,p in enumerate(paramsNames):\n",
    "        for w in selected_walkers:\n",
    "            burn = ax[i].plot(xplot[:nburn], sampler.get_chain()[:nburn,w,i], \n",
    "                              alpha = 0.4, lw = 0.7, zorder = 1)\n",
    "            ax[i].plot(xplot[nburn:], sampler.get_chain(discard=nburn)[:,w,i], \n",
    "                       color=burn[0].get_color(), alpha = 0.8, lw = 0.7, zorder = 1)\n",
    "            \n",
    "            ax[i].set_ylabel(p)\n",
    "            if i==Nparams-1:\n",
    "                ax[i+1].plot(xplot[:nburn], sampler.get_log_prob()[:nburn,w], \n",
    "                             color=burn[0].get_color(), alpha = 0.4, lw = 0.7, zorder = 1)\n",
    "                ax[i+1].plot(xplot[nburn:], sampler.get_log_prob(discard=nburn)[:,w], \n",
    "                             color=burn[0].get_color(), alpha = 0.8, lw = 0.7, zorder = 1)\n",
    "                ax[i+1].set_ylabel('ln P')\n",
    "            \n",
    "    return ax"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "livereveal": {
   "height": 768,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "solarized",
   "width": 1024
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
