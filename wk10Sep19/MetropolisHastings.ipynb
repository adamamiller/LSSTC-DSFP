{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Construction Zone: Building a Metropolis-Hastings Sampler From Scratch\n",
    "\n",
    "#### Version 0.1\n",
    "\n",
    "-----\n",
    "\n",
    "By AA Miller (Northwestern/Adler Planetarium)\n",
    "\n",
    "27 Aug 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we just saw, Bayes' Law provides a framework for understanding/inferring model parameters *given* some observations/data:\n",
    "\n",
    "$$P(\\theta\\mid\\mathbf{x}) = \\frac{P(\\mathbf{x}\\mid\\theta)\\;P(\\theta)}{P(\\mathbf{x})},$$\n",
    "\n",
    "where we have used the short-hand notation with $\\theta$ representing the model parameters, and $\\mathbf{x}$ representing the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To redefine some terms,\n",
    "\n",
    "$P(\\mathbf{x}\\mid\\theta)$, the probability of the data given the model parameters, is the *likelihood*, $\\mathcal{L}$.\n",
    "\n",
    "$P(\\theta)$, the probability of the model parameters, is the *prior*.\n",
    "\n",
    "$P(\\mathbf{x})$ the probability of the data, is the *evidence*. (That sounds weird right? More on this in a bit...)\n",
    "\n",
    "Finally, $P(\\theta\\mid\\mathbf{x})$, the probability of the model parameters given the data, is the *posterior*, $\\pi$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We typically want to estimate some model parameters, $\\theta$, and thus we need to integrate the posterior, $P(\\theta\\mid\\mathbf{x})$.\n",
    "\n",
    "This is **hard**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In particular, how in the world are we supposed to estimate the *probability of data*?!\n",
    "\n",
    "This is – capital H – **Hard**.\n",
    "\n",
    "(We will discuss this on Friday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fortunately, $P(\\mathbf{x})$ simply serves as a normalization term, and, in many applications, can be ignored. \n",
    "\n",
    "Thus, given that the posterior is $\\propto \\mathcal{L}\\;P(\\theta)$, we can maximize the posterior by maximizing the product of the likelihood and the prior. While this still leaves a complicated integral, there are many tools that can approximate that integral rather efficiently..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "including the Metropolis-Hastings algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 1) Simulate the Observations\n",
    "\n",
    "As we work towards implementing the MH sampler, we will attempt to solve a very common problem in the statistical literature: estimating the slope and intercept of a line from noisy data.\n",
    "\n",
    "Here we will simulate some data with known model parameters so we can later compare our estimates to the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 1a**\n",
    "\n",
    "Draw 40 observations between 0 and 100 from a linear model with slope, $m = 2.3$, and intercept, $b = 15$. Assume that each observation is drawn from a Gaussian distribution with mean $\\mu = 0$ and variance, $\\sigma^2$ = 2500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(212)\n",
    "m_true = # complete\n",
    "b_true = # complete\n",
    "x = np.random.uniform(low=0, high=100, size=40)\n",
    "y_true = # complete\n",
    "y_obs = y_true + np.random.normal(0,50,size=40)\n",
    "y_unc = np.ones_like(y_obs)*50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 1b**\n",
    "\n",
    "Plot the observations and the corresponding uncertainties. Overplot a line showing the model from which the data were drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.errorbar( # complete\n",
    "ax.plot([0,100], b_true + m_true*np.array([0,100]))\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 1c**\n",
    "\n",
    "Create a function `get_model_predictions` that calculates the expected value of $\\mathbf{y}$ given input parameters `theta` and positions $\\mathbf{x}$.\n",
    "\n",
    "*Hint* - store the model parameters in a tuple `theta` = (m, b). This will make life easier later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_predictions(theta, x):\n",
    "    '''\n",
    "    Calculate the model at any position x\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : tuple\n",
    "        Model parameters\n",
    "    \n",
    "    x : arr-like, shape (n_samples)\n",
    "        Array of positions where y is measured\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model_preds : arr-like, shape (n_samples)\n",
    "        Array of model values\n",
    "    '''\n",
    "    m, b = theta\n",
    "    model_preds = # complete\n",
    "    \n",
    "    return model_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 2) The Posterior\n",
    "\n",
    "In order to approximate the posterior we need to be able to calculate it at any point within the multidimensional parameter space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Assuming gaussian distributed scatter in the observations, we can describe the probability of any individual observation as a function of the model parameters:\n",
    "\n",
    "$$p(y_i \\mid m, b, x_i, \\sigma_i) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y_i}^2}}\\exp{- \\frac{(y_i - m\\,x_i - b)^2}{2\\sigma_{y_i}^2}},$$\n",
    "\n",
    "where $y_i$ is the $i^\\mathrm{th}$ observation, $x_i$ is the corresponding indpendent position at which $y_i$ is measured, $\\sigma_{y_i}$ is the uncertainty on $y_i$ (constant in this case, but that doesn't have to be the case), and $m$ and $b$ are the slope and intercept for  the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Assuming the observations are independent, then the probability of *all* the observations is the product of the individual probabilities (note – this is the likelihood):\n",
    "\n",
    "$$\\mathcal{L} = \\prod_i p(y_i \\mid m, b, x_i, \\sigma_i) = \\prod_i \\frac{1}{\\sqrt{2\\pi\\sigma_{y_i}^2}}\\exp{- \\frac{(y_i - m\\,x_i - b)^2}{2\\sigma_{y_i}^2}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This product produces really small numbers (individual observations have typical probabilities  < 0.3), which can become numerically unstable, especially for a poor choice of model parameters. \n",
    "\n",
    "A trick to alleviate this issue is to work with the $\\log \\mathcal{L}$, which using a logarithmic identity allows us to trade a product for a sum (computationally much easier!):\n",
    "\n",
    "$$\\log \\mathcal{L} = \\mathcal{K} - \\sum_i \\frac{(y_i - m\\,x_i - b)^2}{2\\sigma_{y_i}^2},$$\n",
    "\n",
    "where $\\mathcal{K}$ is a constant equal to $-\\frac{n}{2}\\log{2\\pi} - \\sum_i \\log{\\sigma_{y_i}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Mathematically, maximizing the likelihood and the log likelihood are identical, and we will leverage this for the remainder of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2a**\n",
    "\n",
    "Write a function `lnlikelihood` to calculate the log likelihood given input parameters `theta`, `y`, `x`, and `y_unc`.\n",
    "\n",
    "*Hint 1* – the constant $\\mathcal{K}$ is the same for any point in the parameter space. Thus it can be ignored as we only care about *relative* differences in the likelihood.\n",
    "\n",
    "*Hint 2* – store the model parameters in a single tuple `theta` = (m, b). This will make life easier later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def lnlikelihood(theta, y, x, y_unc):\n",
    "    '''\n",
    "    Calculate the log likelihood for a model assuming Gaussian uncertainties\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : tuple\n",
    "        Model parameters\n",
    "    \n",
    "    y : arr-like, shape (n_samples)\n",
    "        Array of observational measurements\n",
    "    \n",
    "    x : arr-like, shape (n_samples)\n",
    "        Array of positions where y is measured\n",
    "    \n",
    "    y_unc : arr-like, shape (n_samples)\n",
    "        Array of uncertainties on y\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    lnl : float\n",
    "        The log-likelihood of the observations given the model parameters\n",
    "    '''\n",
    "    model_preds = get_model_predictions( # complete\n",
    "    \n",
    "    lnl = # complete\n",
    "    \n",
    "    return lnl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2b**\n",
    "\n",
    "Confirm your function works by estimating the log likelihood for $m = 1.5$ and $b = 50$.\n",
    "\n",
    "Is the log likelihood higher or lower if you use the true model parameters (see **1a**)? \n",
    "\n",
    "Does this make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('ln L for m=1.5 and b=50 is: {:.4f}'.format(lnlikelihood( # complete\n",
    "print('ln L for m=2.3 and b=15 is: {:.4f}'.format(lnlikelihood( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*write your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The posterior requires an estimate of not just the likelihood, but also the prior. \n",
    "\n",
    "For this problem, how in the world do we choose the prior? \n",
    "\n",
    "(In practice, this can be very complicated and we will discuss this in more detail on Wednesday and Friday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When you have little to no knowledge of the actual model parameters, it is often \"safe\" to assume a wide and flat prior (sometimes referred to as an uniformative prior). A wide and flat prior essentially says that any value within the bounds of the prior is equally likely, while values outside the bounds are unacceptable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While we know the true answer, let's pretend that we don't. From the data (technically, using the data to estimate the prior is cheating, as we'll discuss on Friday) we can see that the slope is definitely positive, and also definitely less than 300 (the largest y value in the data set). Thus, we will set a uniform prior on $m$ from 0 to 300:\n",
    "\n",
    "$$P(m) \\sim \\mathcal{U}(0,300).$$\n",
    "\n",
    "From similar arguments the intercept is likely somewhere between -100 and 100:\n",
    "\n",
    "$$P(b) \\sim \\mathcal{U}(-100,100).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2c**\n",
    "\n",
    "Write a function `lnprior` to calculate the log of the prior from an input tuple of the model parameters `theta`.\n",
    "\n",
    "*Hint* - while the $\\log{0}$ is undefined, have the function return `-np.inf` for values outside the bounds of the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def lnprior(theta):\n",
    "    '''\n",
    "    Calculate the log of the prior for the model parameters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : tuple\n",
    "        Model parameters\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    lnp : float\n",
    "        The log-prior of the model parameters\n",
    "    '''\n",
    "    m, b = theta\n",
    "    if 0 <= m <= 300 and -100 <= b <= 100:\n",
    "        lnp = np.log(1/300) + np.log(1/200) # assumes P(m) and P(y) are completely independent\n",
    "    else:\n",
    "        lnp = # complete\n",
    "    \n",
    "    return lnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2d**\n",
    "\n",
    "Calculate the prior for $(m, b) = (1, 0)$, $(2.3, 15)$, $(3, -150)$, and $(-1, 90)$.\n",
    "\n",
    "Do your answers make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('ln P for m=1.0 and b=0 is: {:.4f}'.format(# complete\n",
    "print('ln P for m=2.3 and b=15 is: {:.4f}'.format(# complete\n",
    "print('ln P for m=3 and b=-150 is: {:.4f}'.format(# complete\n",
    "print('ln P for m=-1 and b=90 is: {:.4f}'.format(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*write your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2e**\n",
    "\n",
    "Write a function `lnposterior` to calculate the log of the posterior.\n",
    "\n",
    "*Hint* - this is straightforward using functions you have already written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def lnposterior(theta, y, x, y_unc):\n",
    "    '''\n",
    "    Calculate the log posterior\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : tuple\n",
    "        Model parameters\n",
    "    \n",
    "    y : arr-like, shape (n_samples)\n",
    "        Array of observational measurements\n",
    "    \n",
    "    x : arr-like, shape (n_samples)\n",
    "        Array of positions where y is measured\n",
    "    \n",
    "    y_unc : arr-like, shape (n_samples)\n",
    "        Array of uncertainties on y\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    lnpost : float\n",
    "        The log-posterior from the observations and model parameters\n",
    "    '''\n",
    "    lnp = lnprior(theta)\n",
    "    if not np.isfinite(lnp):\n",
    "        return -np.inf\n",
    "    lnl = # complete\n",
    "    lnpost = # complete\n",
    "        \n",
    "    return lnpost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2f**\n",
    "\n",
    "What is the log posterior for $m = 1.5$ and $b = 50$?\n",
    "\n",
    "What is the log posterior for $m = 2.3$ and $b = 15$?\n",
    "\n",
    "What is the log posterior for $m = 1.5$ and $b = 200$?\n",
    "\n",
    "Does this make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('ln posterior for m=1.5 and b=50 is: {:.4f}'.format( # complete\n",
    "print('ln posterior for m=2.3 and b=15 is: {:.4f}'.format( # complete\n",
    "print('ln posterior for m=1.5 and b=200 is: {:.4f}'.format( # complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*write your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 3) The Metropolis-Hastings Algorithm\n",
    "\n",
    "The [Metropolis-Hastings Algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) (MH) is the most simple [Markov Chain](https://en.wikipedia.org/wiki/Markov_chain) [Monte Carlo](https://en.wikipedia.org/wiki/Monte_Carlo_method) ([MCMC](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)) procedure for estimating Bayesian posteriors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will cover the nitty gritty (i.e., mathematics) of how MCMC works tomorrow. For now, we will focus on Metropolis-Hastings as an algorithm, and assume it works.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The pseudo-code for the MH algorithm is:\n",
    "\n",
    "0. pick some position $\\theta_0$ in the parameter space and calculate the posterior $P(\\theta_{0}\\mid \\mathbf{x})$\n",
    "1. begin the chain\n",
    "    - \"propose\" a move from the current position $\\theta_{i}$ to a new position $\\theta_{i+1}$\n",
    "    - calculate the posterior at $\\theta_{i+1}$, $P(\\theta_{i+1}\\mid \\mathbf{x})$\n",
    "    - draw a random number, $R \\sim \\mathcal{U}(0,1)$\n",
    "    - if the ratio $P(\\theta_{i+1}\\mid \\mathbf{x})/P(\\theta_{i}\\mid \\mathbf{x})$ is $> R$, \"accept\" the proposed move and advance the chain to $\\theta_{i+1}$\n",
    "    - else \"reject\" the proposal and set $\\theta_{i+1} = \\theta_{i}$\n",
    "\n",
    "2. repeat until chain is \"finished\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A few subtleties worth noting: the use of the random number is essential for allowing the MH algorithm to \"explore\" the full posterior. It's actually possible to add links to the chain that are less probable than the current position. This is an important distinction relative to optimization routines that simply identify local maxima, but only progressing in the direction of increased probability.\n",
    "\n",
    "Also \"finished\" isn't really defined - we will cover this tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An extra subtle point: technically you need to calculate the full Hastings ratio, $H$, when evaluating whether or not to accept or reject a proposal:\n",
    "\n",
    "$$H = \\frac{\\pi(\\theta_{i+1})}{\\pi(\\theta_{i})} \\frac{q(\\theta_i \\mid \\theta_{i+1})}{q(\\theta_{i+1} \\mid \\theta_{i})},$$\n",
    "\n",
    "where $q(\\theta_i \\mid \\theta_{i+1})/q(\\theta_{i+1} \\mid \\theta_{i})$ is the ratio of transition probabilities from $\\theta_{i+1}$ to $\\theta_{i}$ and back again. \n",
    "\n",
    "Fortunately, most proposals are symmetric, including what we will implement, meaning the ratio of transition probabilities is 1, and thus, this term can be safely ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3a**\n",
    "\n",
    "Write a function `hastings_ratio` to calculate the ratio of log-posterior values at `theta_1` relative to `theta_0`.\n",
    "\n",
    "*Hint* – assume the ratio of transition probabilities is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def hastings_ratio(theta_1, theta_0, y, x, y_unc):\n",
    "    '''\n",
    "    Calculate the Hastings ratio\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta_1 : tuple\n",
    "        proposed new posterior position \n",
    "    \n",
    "    theta_0 : tuple\n",
    "        current posterior position\n",
    "    \n",
    "    y : arr-like, shape (n_samples)\n",
    "        Array of observational measurements\n",
    "    \n",
    "    x : arr-like, shape (n_samples)\n",
    "        Array of positions where y is measured\n",
    "    \n",
    "    y_unc : arr-like, shape (n_samples)\n",
    "        Array of uncertainties on y\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    h_ratio : float\n",
    "        The Hastings ratio\n",
    "    '''\n",
    "    lnpost1 = # complete\n",
    "    lnpost0 = # complete\n",
    "    \n",
    "    h_ratio = # complete\n",
    "    \n",
    "    return h_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3b**\n",
    "\n",
    "Calculate the Hastings ratio to move from $(2.1, 50)$ to $(2.2,30)$.\n",
    "\n",
    "How often would the algorithm accept this step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('The Hastings ratio is: {:.4f}'.format( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*write your answer here*\n",
    "\n",
    "In this case $H > 1$, and thus this proposed move  would be accepted every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3c** \n",
    "\n",
    "What about the reverse proposal? Calculate the Hastings ratio to move from (2.2, 30) to (2.1, 50). \n",
    "\n",
    "On average, how many times would such a proposal need to be made to be accepted by the MH algortihm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('The Hastings ratio is: {:.4f}'.format( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*write your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we will show below, the difference between an MCMC chain that takes forever and one that samples the posterior \"quickly\" is largely a function of the proposal distribution. \n",
    "\n",
    "We want a symmetric proposal to ensure the transition ratio is equal to 1. The easiest way to accomplish this is with a normal distribution centered at the current position in the posterior, with a user-defined covariance matrix. (Today we will assume the covariance matrix is  diagonal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 3d**\n",
    "\n",
    "Write a function `propose_jump` that creates new proposal positions for the MCMC chain, based on its current position, `theta`, and a covariance matrix, `cov`, for the multivariate normal distribution centered on `theta`.\n",
    "\n",
    "*Hint 1* - allow `cov` to be either 1-D with standard deviations as the input, in which case the entries are converted to a diagonal matrix, or 2-D, where the full covariance matrix is specified.\n",
    "\n",
    "*Hint 2* - you may find [`numpy.random.multivariate_normal`](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.multivariate_normal.html) helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def propose_jump(theta, cov):\n",
    "    '''\n",
    "    Generate a proposed new position for MCMC chain\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : 1-D array_like, of length N\n",
    "        current position of the MCMC chain\n",
    "    \n",
    "    cov : 1-D or 2-D array_like, of shape (N,) or (N, N)\n",
    "        Covariance matrix of the distribution. It must be symmetric \n",
    "        and positive-semidefinite for proper sampling.\n",
    "        \n",
    "        1-D inputs for cov require the standard deviation along \n",
    "        each axis of the N-dimensional Gaussian.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    proposed_position : 1-D array_like, of length N\n",
    "    '''\n",
    "    if np.shape(theta) == np.shape(cov):\n",
    "        cov = np.diag(np.array(cov)**2)\n",
    "    \n",
    "    proposed_position = # complete\n",
    "    \n",
    "    return proposed_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3e**\n",
    "\n",
    "Propose 5 random steps from the position $(m, b) = (2, 45)$, and calculate the Hastings ratio for each of the 5 proposals. Use a standard deviation of 1 along both axes of the multivariate normal distribution.\n",
    "\n",
    "Do you notice anything interesting about your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "theta_0 = (2,45)\n",
    "for _ in range(5):\n",
    "    new_pos = propose_jump(theta_0, (1,1))\n",
    "    H = hastings_ratio( # complete\n",
    "    print('Jumping to {}, gives H = {:.4f}'.format(new_pos, H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*write your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3f**\n",
    "\n",
    "Repeat **3e** but this time use a standard deviation of 0.1 for the slope and 10 for intercept.\n",
    "\n",
    "Are the results substantially different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "theta_0 = (2,45)\n",
    "for _ in range(5):\n",
    "    new_pos = propose_jump(theta_0, (0.1,10))\n",
    "    H = hastings_ratio( # complete\n",
    "    print('Jumping to {}, gives H = {:.4f}'.format(new_pos, H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*write your answer here*\n",
    "\n",
    "The Hastings ratios are now much larger. It is clear that the width of the multivariate normal along each axis will affect the efficiency of the MH sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Referring back to the pseudo code above, we now have all the ingredients necessary to put together a MH sampler. We just need to stitch everything together so that we actually form a chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3g**\n",
    "\n",
    "Write a function `mh_mcmc` that takes an initial position `theta_0`, covariance matrix for the multivariate normal proposal distribution `cov`, total number of steps `nsteps`, and observations `y`, `x`, and `y_unc` as inputs and returns the Metropolis-Hastings chain for the given inputs.\n",
    "\n",
    "The output \"chain\" should include the position of the chain within the parameter space, the log of the posterior at that position, and the \"acceptance ratio\" of the chain until that point. The acceptance ratio is the fraction of previously proposed steps (including the current proposal) that have been accepted. We will come back to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def mh_mcmc(theta_0, cov, nsteps, y, x, y_unc):\n",
    "    '''\n",
    "    Metropolis-Hastings MCMC algorithm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta_0 : 1-D array_like of shape N\n",
    "        starting position for the MCMC chain\n",
    "    \n",
    "    cov : 1-D or 2-D array_like, of shape (N,) or (N, N)\n",
    "        Covariance matrix of the distribution. It must be symmetric \n",
    "        and positive-semidefinite for proper sampling.\n",
    "        \n",
    "        1-D inputs for cov require the standard deviation along \n",
    "        each axis of the N-dimensional Gaussian.\n",
    "\n",
    "    nsteps : int\n",
    "        Number of steps to take in the MCMC chain\n",
    "        \n",
    "    y : arr-like, shape (n_samples)\n",
    "        Array of observational measurements\n",
    "    \n",
    "    x : arr-like, shape (n_samples)\n",
    "        Array of positions where y is measured\n",
    "    \n",
    "    y_unc : arr-like, shape (n_samples)\n",
    "        Array of uncertainties on y\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    (positions, lnpost_at_pos, acceptance_ratio) : tuple\n",
    "        positions : 2-D array_like of shape (nsteps+1, N)\n",
    "            Position of the MCMC chain at every step\n",
    "        \n",
    "        lnpost_at_pos : 1-D array_like of shape nsteps+1\n",
    "            log-posterior value at the position of the MCMC chain\n",
    "        \n",
    "        acceptance_ratio : 1-D array_like of shape nsteps+1\n",
    "            acceptance ratio of all previous steps in the chain    \n",
    "    '''\n",
    "    \n",
    "    positions = np.zeros((nsteps+1, len(theta_0)))\n",
    "    lnpost_at_pos = -np.inf*np.ones(nsteps+1)\n",
    "    acceptance_ratio = np.zeros_like(lnpost_at_pos)\n",
    "    accepted = 0\n",
    "    \n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    \n",
    "    \n",
    "    return (positions, lnpost_at_pos, acceptance_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3h**\n",
    "\n",
    "Run the MH sampler! \n",
    "\n",
    "Using a starting point of $(m, b) = (2.7, -70)$ (this is estimated from the two most extreme points in the dataset), `cov = (0.25, 5)`, and run the chain for 500 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pos, lnpost, acc = mh_mcmc((2.7,-70), (0.25, 5), 500, y_obs, x, y_unc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3i**\n",
    "\n",
    "Plot the positions of the chain in the $(m, b)$ plane. Overplot the true answer as a five-pointed star.\n",
    "\n",
    "Does your plot \"make sense\"?\n",
    "\n",
    "*Hint* – connect the points with line segments so you can visualize its evolution. \n",
    "\n",
    "*Hint 2* – set `alpha = 0.4` to get a sense of positions where multiple proposals were required for the chain to move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot( # complete\n",
    "ax.plot(# complete, \n",
    "        '*', ms=30, \n",
    "        mfc='Crimson', mec='0.8', mew=2, \n",
    "        alpha=0.7)\n",
    "ax.set_xlabel('m', fontsize=14)\n",
    "ax.set_ylabel('b', fontsize=14)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3j**\n",
    "\n",
    "Recreate the above plot, but this time color code the individual points by the value of the log posterior at each position.\n",
    "\n",
    "*Hint* –– you will want to use [`pyplot.scatter`](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.scatter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(pos[:,0], pos[:,1], 'o-', alpha=0.3)\n",
    "ax.scatter( # complete\n",
    "ax.plot(# complete, \n",
    "        '*', ms=30, \n",
    "        mfc='Crimson', mec='0.8', mew=2, \n",
    "        alpha=0.7, zorder=20)\n",
    "ax.set_xlabel('m', fontsize=14)\n",
    "ax.set_ylabel('b', fontsize=14)\n",
    "cbar = fig.colorbar(cax)\n",
    "cbar.ax.set_ylabel(r'$\\log \\; \\pi (\\theta)$', fontsize=12)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Awesome! \n",
    "\n",
    "You are now tooled up to solve any Bayesian problem that you might encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 4) Optimizing MH\n",
    "\n",
    "While you are now, no doubt, wildly excited about what you have accomplished in $\\sim{90}$ min time, there should be a few things that bother you. In particular, there were several \"hard coded\" options in the previous problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Weird Choice #1\n",
    "\n",
    "Why 500 steps? \n",
    "\n",
    "It's clear from **3i** that ~10-20 steps is no where near enough, as the chain does not sample the maximum of the posterior. But is 500 steps too many? Or is it not enough?\n",
    "\n",
    "(There are mathematical ways to judge this, which we will cover tomorrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Weird Choice #2\n",
    "\n",
    "Why `(0.2, 10)` for the widths of the multivariate normal proposal function?\n",
    "\n",
    "Previously we argued that it's possible to take steps that are too large, but is it also possible the steps are too small?\n",
    "\n",
    "(Technically, as we will see tomorrow, the step size does not matter as all MCMC chains will eventually eplore the full posterior within an infinite number of steps, but who amoung us can wait infinity seconds to learn the slope of this line?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Weird Choice #3\n",
    "\n",
    "Why did we start the chain at (2.7,-70)?\n",
    "\n",
    "(Again, technically this does not matter, but do you want to wait for an infinite number of steps before getting the results?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tommorrow we will formalize all of these choices, for now we will try to develop some intuition via some graphical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4a**\n",
    "\n",
    "Write a function `plot_post` that recreates the plots from **3i** and **3j** side by side given inputs \n",
    "`theta_0`, `cov`, `nsteps`, `y`, `x`, and `y_unc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_post(theta_0, cov, nsteps, y, x, y_unc):\n",
    "    '''\n",
    "    Plot posterior trace from MH MCMC\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta_0 : 1-D array_like of shape N\n",
    "        starting position for the MCMC chain\n",
    "    \n",
    "    cov : 1-D or 2-D array_like, of shape (N,) or (N, N)\n",
    "        Covariance matrix of the distribution. It must be symmetric \n",
    "        and positive-semidefinite for proper sampling.\n",
    "        \n",
    "        1-D inputs for cov require the standard deviation along \n",
    "        each axis of the N-dimensional Gaussian.\n",
    "\n",
    "    nsteps : int\n",
    "        Number of steps to take in the MCMC chain\n",
    "        \n",
    "    y : arr-like, shape (n_samples)\n",
    "        Array of observational measurements\n",
    "    \n",
    "    x : arr-like, shape (n_samples)\n",
    "        Array of positions where y is measured\n",
    "    \n",
    "    y_unc : arr-like, shape (n_samples)\n",
    "        Array of uncertainties on y\n",
    "    '''\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    # complete\n",
    "    \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4b**\n",
    "\n",
    "Plot the MH chain using `theta_0 = (2.7, -70)`, `cov = (0.25, 5)`, for 50 steps.\n",
    "\n",
    "Do the same for a chain with 5000 steps.\n",
    "\n",
    "What changes do you notice as you vary the number of steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 50 step plot\n",
    "plot_post( # complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 5000 step plot\n",
    "plot_post( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*write your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4b**\n",
    "\n",
    "Plot the MH chain using `theta_0 = (2.7, -70)`, `cov = (.025, .5)`, for 500 steps.\n",
    "\n",
    "Do the same for `cov = (2.5, 50)`.\n",
    "\n",
    "What changes do you notice as you vary the size of the proposal distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# cov = (0.025, .5) plot\n",
    "plot_post( # complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# cov = (2.5, 50) plot\n",
    "plot_post( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*write your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4c**\n",
    "\n",
    "Plot the MH chain using `theta_0 = (27, -100)`, `cov = (.25, 5)`, for 500 steps.\n",
    "\n",
    "Do the same for `theta_0 = (.27, -7)`.\n",
    "\n",
    "What changes do you notice as you vary the starting position of the chain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# theta_0 = (27, -100) plot\n",
    "plot_post( # complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# theta_0 = (.27, -7) plot\n",
    "plot_post( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*write your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When evaluating MCMC chains, there are many different diagnostic plots that are helpful for that purpose. \n",
    "\n",
    "Below is a helper function that overplots draws from the posterior on top of the observations, an estimate of the posterior, and the 1-d chains for $m$, $b$, $\\log \\pi$, and the acceptance rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mh_summary(theta_0, cov, nsteps, y, x, y_unc):\n",
    "    '''\n",
    "    Plot the posterior, draws from the posterior, and 1-d chains\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta_0 : 1-D array_like of shape N\n",
    "        starting position for the MCMC chain\n",
    "    \n",
    "    cov : 1-D or 2-D array_like, of shape (N,) or (N, N)\n",
    "        Covariance matrix of the distribution. It must be symmetric \n",
    "        and positive-semidefinite for proper sampling.\n",
    "        \n",
    "        1-D inputs for cov require the standard deviation along \n",
    "        each axis of the N-dimensional Gaussian.\n",
    "\n",
    "    nsteps : int\n",
    "        Number of steps to take in the MCMC chain\n",
    "        \n",
    "    y : arr-like, shape (n_samples)\n",
    "        Array of observational measurements\n",
    "    \n",
    "    x : arr-like, shape (n_samples)\n",
    "        Array of positions where y is measured\n",
    "    \n",
    "    y_unc : arr-like, shape (n_samples)\n",
    "        Array of uncertainties on y\n",
    "    '''\n",
    "    pos, lnpost, acc = mh_mcmc(theta_0, cov, nsteps, y_obs, x, y_unc)\n",
    "\n",
    "    fig = plt.figure(figsize=(7.5,6))\n",
    "    ax1 = plt.subplot2grid((4,5), (0, 0), colspan=2, rowspan=2)\n",
    "    ax2 = plt.subplot2grid((4,5), (2, 0), colspan=2, rowspan=2)\n",
    "    ax3 = plt.subplot2grid((4,5), (0, 2), colspan=3)\n",
    "    ax4 = plt.subplot2grid((4,5), (1, 2), colspan=3, sharex=ax3)\n",
    "    ax5 = plt.subplot2grid((4,5), (2, 2), colspan=3, sharex=ax3)\n",
    "    ax6 = plt.subplot2grid((4,5), (3, 2), colspan=3, sharex=ax3)\n",
    "\n",
    "    # posterior\n",
    "    ax1.hexbin(pos[:,0], pos[:,1], gridsize=50, mincnt=1, bins='log')\n",
    "    ax1.plot(2.3, 15, '*', ms=30, \n",
    "        mfc='Crimson', mec='0.8', mew=2, \n",
    "        alpha=0.7)\n",
    "    ylims = ax1.get_ylim()\n",
    "    xlims = ax1.get_xlim()\n",
    "    ax1.plot([2.3, 2.3], ylims, 'Crimson', alpha=0.3)\n",
    "    ax1.plot(xlims, [15, 15], 'Crimson', alpha=0.3)\n",
    "    ax1.set_ylim(ylims)\n",
    "    ax1.set_xlim(xlims)\n",
    "    ax1.set_xlabel('m')\n",
    "    ax1.set_ylabel('b')\n",
    "    ax1.xaxis.set_ticks_position('top')\n",
    "    ax1.xaxis.set_label_position('top')\n",
    "    ax1.tick_params(top=True, bottom=False)\n",
    "    \n",
    "    # posterior draws\n",
    "    ax2.errorbar(x, y_obs, y_unc, fmt='o')\n",
    "#     ax2.plot([0,100], \n",
    "#              b_true + m_true*np.array([0,100]),\n",
    "#              '--', color='DarkOrange', lw=2, zorder=-10)\n",
    "    for draw in np.random.choice(len(pos), 10, replace=False):\n",
    "        ax2.plot([0,100], pos[draw,1] + pos[draw,0]*np.array([0,100]),\n",
    "                 'DarkOrange', alpha=0.4)\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')\n",
    "    \n",
    "    ax3.plot(pos[:,0])\n",
    "    ax3.set_ylabel('m')\n",
    "    \n",
    "    ax4.plot(pos[:,1])\n",
    "    ax4.set_ylabel('b')\n",
    "\n",
    "    ax5.plot(lnpost)\n",
    "    ax5.set_ylabel('$\\ln \\; \\pi$')\n",
    "\n",
    "    ax6.plot(acc)\n",
    "    ax6.set_ylabel('acceptance')\n",
    "    ax6.set_xlabel('step number')\n",
    "    plt.setp(ax3.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax4.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax5.get_xticklabels(), visible=False)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.93, left=0.09, right=0.99, hspace=0.07, wspace=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4d**\n",
    "\n",
    "Create a summary plot using our \"default\" parameters of `theta_0 = (2.7, -70)`, `cov = (0.25, 5)`, and 500 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_mh_summary( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Without proof, I will state that we are looking for the following as we diagnose our MCMC:\n",
    "\n",
    "1. dense sampling of the high probability portion of the posterior\n",
    "2. draws from the posterior that are mostly consistent with the data\n",
    "3. trace plots for m and b that are roughly i.i.d. (and do not show significant structure)\n",
    "4. log posterior values that oscillate around the maximum with variations no larger than a few\n",
    "5. a stable acceptance ratio of ~0.5 (note for simple problems 0.5 is good, for complex posteriors 0.25 is more likely)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4e**\n",
    "\n",
    "Vary the MH MCMC parameters to \"better\" sample the posterior according to the criteria given above. \n",
    "\n",
    "*Hint* – this will likely take a lot of trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_mh_summary( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenge Problem) Inference & Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Challenge 1a**\n",
    "\n",
    "Based on your solution to the previous problem, what do you estimate is the value of $m$ and $b$? \n",
    "\n",
    "What are the marginalized 90% credible regions for these paramters? How does this compare to the true answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# complete\n",
    "\n",
    "print('m = {0[0]:.3f} with a 90% credible region from {0[1]:.3f} to {0[2]:.3f}'.format( # complete\n",
    "print('b = {0[0]:.3f} with a 90% credible region from {0[1]:.3f} to {0[2]:.3f}'.format( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Challenge 1b**\n",
    "\n",
    "How does your answer compare to the maximum-likelihood method? Estimate the values of $m$ and $b$ using [`scipy.optimize`](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html).\n",
    "\n",
    "*Hint* –– you'll want to minimize the *negative* log likelihood, in order to estimate the optimal parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Challenge 1c**\n",
    "\n",
    "Based on your solutions above, what do you estimate is the value of y at x = 50?\n",
    "\n",
    "What is the 90% credible region for this prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenge Problem 2) Gibbs Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The [Gibbs sampler](https://en.wikipedia.org/wiki/Gibbs_sampling) is a special case of the Metropolis-Hastings sampler.\n",
    "\n",
    "Alter the Metropolis-Hastings algorithm that you have created to act as a Gibbs sampler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
